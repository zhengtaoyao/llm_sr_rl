#!/usr/bin/env python3
"""
GPUé…ç½®æµ‹è¯•è„šæœ¬
éªŒè¯æ˜¯å¦èƒ½æ­£ç¡®ä½¿ç”¨æŒ‡å®šçš„GPUå¡
"""

import os
import torch

def test_gpu_config():
    """æµ‹è¯•GPUé…ç½®"""
    print("ğŸ” GPUé…ç½®æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES')
    print(f"CUDA_VISIBLE_DEVICES: {cuda_visible}")
    
    # æ£€æŸ¥PyTorchæ˜¯å¦å¯ç”¨CUDA
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA device count: {torch.cuda.device_count()}")
        
        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„GPU
        for i in range(torch.cuda.device_count()):
            device = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {device.name} ({device.total_memory / 1024**3:.1f} GB)")
        
        # æµ‹è¯•GPUå†…å­˜
        print("\nğŸ§ª æµ‹è¯•GPUå†…å­˜ä½¿ç”¨:")
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            print(f"GPU {i}: {allocated:.1f} GB allocated, {cached:.1f} GB cached")
    
    print("âœ… GPUé…ç½®æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_gpu_config() 