"""
GRPO Training Runner for LLM-SR Integration

This module provides the main training loop that integrates VERL's GRPO
with LLM-SR's evaluation system for symbolic regression tasks.

ğŸ”¥ é‡è¦è¯´æ˜ï¼š
- train_llmsr_grpo_direct(): ç›´è¿æ¨¡å¼ï¼ŒActor è¿›ç¨‹ç›´æ¥åŠ è½½æ¨¡å‹å¹¶æ›´æ–°æƒé‡
- train_llmsr_grpo_http(): HTTP æ¨¡å¼ï¼Œé€šè¿‡ HTTP è°ƒç”¨å¤–éƒ¨ LLMï¼Œä¸æ›´æ–°æƒé‡
- é»˜è®¤ä½¿ç”¨ç›´è¿æ¨¡å¼è¿›è¡ŒçœŸæ­£çš„æƒé‡å¾®è°ƒ
"""

# ğŸ”¥ ä¿®å¤ vLLM å†…å­˜æ± å…¼å®¹æ€§é—®é¢˜ï¼šåœ¨å¯¼å…¥PyTorch/vLLMä¹‹å‰è®¾ç½®CUDAåˆ†é…å™¨é…ç½®
import os
# ç§»é™¤ expandable_segments ä»¥é¿å… vLLM å†…å­˜æ± æ–­è¨€å¤±è´¥
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,garbage_collection_threshold:0.6"
import sys
from pathlib import Path
from typing import Dict, Any
from omegaconf import OmegaConf, DictConfig
import tempfile

# Add VERL to path if needed
verl_path = str(Path(__file__).parent.parent.parent.parent / "verl")
if verl_path not in sys.path:
    sys.path.append(verl_path)

from verl.trainer.main_ppo import run_ppo
from llmsr.rl.grpo_worker import compute_llmsr_reward



def create_llmsr_dataset(template_path: str, data_path: str, output_dir: str):
    """
    Create VERL-compatible dataset from LLM-SR specification and data.
    
    Args:
        template_path: Path to LLM-SR specification template
        data_path: Path to training data CSV
        output_dir: Directory to save converted dataset
        
    Returns:
        Path to the created dataset file
    """
    import pandas as pd
    import json
    
    # Load the specification template
    with open(template_path, 'r') as f:
        specification = f.read()
    
    # Extract the prompt template (everything before @equation.evolve function)
    lines = specification.split('\n')
    prompt_lines = []
    in_evolve_function = False
    
    for line in lines:
        if '@equation.evolve' in line:
            in_evolve_function = True
            continue
        if in_evolve_function and line.strip().startswith('def '):
            # Found the function definition, include it and stop
            prompt_lines.append(line.rstrip())
            break
        if not in_evolve_function:
            prompt_lines.append(line.rstrip())
    
    # Create the prompt template
    base_prompt = '\n'.join(prompt_lines).strip()
    
    # Load data to determine the task
    df = pd.read_csv(data_path)
    # num_samples = min(1000, len(df))  # Limit for training efficiency
    # num_samples = min(5000, len(df))  # Limit for training efficiency
    num_samples = min(len(df), len(df))  # Limit for training efficiency

    # Create dataset entries in chat format
    dataset_entries = []
    for i in range(num_samples):
        # Format as conversation with user role (required by chat models)
        chat_prompt = [
            {
                "role": "user", 
                "content": base_prompt
            }
        ]
        
        entry = {
            "prompt": chat_prompt,
            "data_source": "llm_sr_train",
            "reward_model": {
                "style": "rule"  # Use rule-based evaluation
            }
        }
        dataset_entries.append(entry)
    
    # Save as parquet file (VERL format)
    import pyarrow as pa
    import pyarrow.parquet as pq
    
    output_path = os.path.join(output_dir, "llmsr_train.parquet")
    table = pa.Table.from_pylist(dataset_entries)
    pq.write_table(table, output_path)
    
    print(f"âœ… Created VERL dataset: {output_path} ({len(dataset_entries)} entries)")
    return output_path


def create_llmsr_reward_file(template_path: str, data_path: str, output_dir: str):
    """
    Create a custom reward function file for VERL.
    
    Args:
        template_path: Path to LLM-SR specification template
        data_path: Path to training data CSV  
        output_dir: Directory to save reward function file
        
    Returns:
        Path to the created reward function file
    """
    
    reward_function_code = f'''
"""
ğŸ”¥ ç®€åŒ–çš„VERLå¥–åŠ±å‡½æ•° - å…¼å®¹æ–°æ•°æ®æ ¼å¼
é¿å…KeyError: 'ground_truth'é—®é¢˜
"""

import sys
import os
from pathlib import Path

# å¯¼å…¥ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
sys.path.append(str(Path(__file__).parent.parent.parent))
from simple_verl_reward import compute_score as simple_compute_score

def compute_score(data_sources=None, solution_strs=None, ground_truths=None, extra_infos=None, **kwargs):
    """
    ğŸ”¥ ä½¿ç”¨ç®€åŒ–çš„å¥–åŠ±è®¡ç®—ï¼Œé¿å…ground_truth KeyError - æ”¯æŒé»˜è®¤å‚æ•°
    
    Args:
        data_sources: æ•°æ®æºåˆ—è¡¨ (é»˜è®¤: None)
        solution_strs: æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå­—ç¬¦ä¸²åˆ—è¡¨ (é»˜è®¤: None)
        ground_truths: å‚è€ƒç­”æ¡ˆåˆ—è¡¨ (é»˜è®¤: None)
        extra_infos: é¢å¤–ä¿¡æ¯åˆ—è¡¨ (é»˜è®¤: None)
        
    Returns:
        rewards: å¥–åŠ±åˆ†æ•°åˆ—è¡¨
    """
    
    print(f"ğŸ”§ åŒ…è£…å™¨è¢«è°ƒç”¨ï¼Œå‚æ•°ç±»å‹: data_sources={{type(data_sources)}}, solution_strs={{type(solution_strs)}}, ground_truths={{type(ground_truths)}}, extra_infos={{type(extra_infos)}}")
    
    # ğŸ”§ å¤„ç†Noneå‚æ•°
    if data_sources is None:
        data_sources = []
    if solution_strs is None:
        solution_strs = []
    if ground_truths is None:
        ground_truths = []
    if extra_infos is None:
        extra_infos = []
    
    # ç¡®å®šé—®é¢˜ç±»å‹
    problem_type = "oscillator1"  # é»˜è®¤å€¼
    data_path = "{data_path}"
    
    if "oscillator1" in data_path:
        problem_type = "oscillator1"
    elif "oscillator2" in data_path:
        problem_type = "oscillator2"
    elif "bactgrow" in data_path:
        problem_type = "bactgrow"
    elif "stressstrain" in data_path:
        problem_type = "stressstrain"
    
    # æ„å»ºextra_infosä»¥ä¼ é€’é—®é¢˜ç±»å‹
    if not extra_infos:
        extra_infos = [{{'problem_type': problem_type}}] * len(solution_strs)
    else:
        # ç¡®ä¿æ¯ä¸ªextra_infoéƒ½æœ‰problem_type
        for i, extra_info in enumerate(extra_infos):
            if not extra_info:
                extra_infos[i] = {{'problem_type': problem_type}}
            elif 'problem_type' not in extra_info:
                extra_infos[i]['problem_type'] = problem_type
    
    # è°ƒç”¨ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
    return simple_compute_score(data_sources, solution_strs, ground_truths, extra_infos, **kwargs)
'''
    
    output_path = os.path.join(output_dir, "llmsr_reward.py")
    with open(output_path, 'w') as f:
        f.write(reward_function_code)
    
    print(f"âœ… Created reward function: {output_path}")
    return output_path


def create_grpo_config_direct(
    model_path: str,
    dataset_path: str, 
    reward_file_path: str,
    output_dir: str,
    **kwargs
) -> DictConfig:
    """
    ğŸ”¥ ä¿®å¤æ‰¹é‡å¤§å°é…ç½®çš„ç›´è¿æ¨¡å¼ VERL GRPO é…ç½®
    
    æ ¹æ®VERLå®˜æ–¹ç¤ºä¾‹ï¼Œæ­£ç¡®è®¾ç½®æ‰¹é‡å¤§å°é¿å…é™¤é›¶é”™è¯¯
    """
    
    # ğŸ”¥ æ‰¹é‡å¤§å°é…ç½® - å‚ç…§VERLå®˜æ–¹ç¤ºä¾‹ï¼Œé’ˆå¯¹7Bæ¨¡å‹ä¼˜åŒ–å†…å­˜
    gpus = kwargs.get('gpus', 6)
    micro_batch_size_per_gpu = 1  # ğŸ”¥ å‡å°‘åˆ°1ä»¥èŠ‚çœå†…å­˜
    rollout_n = kwargs.get('rollout_n', 4)  # å“åº”æ•°é‡
    
    # è®¡ç®—å„çº§æ‰¹é‡å¤§å° (å‚ç…§VERLå®˜æ–¹å…¬å¼ï¼Œä½†ä½¿ç”¨æ›´å°çš„å€¼)
    traj_micro_bsz = micro_batch_size_per_gpu * gpus          # 1 * 6 = 6
    traj_mini_bsz = traj_micro_bsz * 2                        # 6 * 2 = 12  
    prompt_mini_bsz = traj_mini_bsz * rollout_n               # 12 * 4 = 48
    prompt_bsz = prompt_mini_bsz * 1                          # 48 * 1 = 48 (å‡å°‘å€æ•°)
    
    print(f"ğŸ”§ å†…å­˜ä¼˜åŒ–æ‰¹é‡å¤§å°é…ç½®:")
    print(f"  å¾®æ‰¹é‡/GPU: {micro_batch_size_per_gpu}")
    print(f"  GPUæ•°é‡: {gpus}")
    print(f"  è½¨è¿¹å¾®æ‰¹é‡: {traj_micro_bsz}")
    print(f"  è½¨è¿¹å°æ‰¹é‡: {traj_mini_bsz}")
    print(f"  æç¤ºå°æ‰¹é‡: {prompt_mini_bsz}")
    print(f"  è®­ç»ƒæ‰¹é‡: {prompt_bsz}")
    print(f"  ğŸ”¥ å†…å­˜ä¼˜åŒ–: å¯ç”¨å‚æ•°/ä¼˜åŒ–å™¨offload")
    
    # ç›´è¿æ¨¡å¼ GRPO é…ç½®
    config = {
        # ç®—æ³•é…ç½®
        "algorithm": {
            "adv_estimator": "grpo",
            "gamma": 1.0,
            "lam": 1.0,
            "norm_adv_by_std_in_grpo": True,
            "use_kl_in_reward": False,
            "kl_ctrl": {
                "type": "fixed",
                "kl_coef": 0.001,
                "horizon": 10000,
                "target_kl": 0.1
            }
        },
        
        # æ•°æ®é…ç½®
        "data": {
            "train_files": [dataset_path],
            "val_files": [dataset_path],
            "train_batch_size": prompt_bsz,  # ğŸ”¥ ä½¿ç”¨è®¡ç®—å¾—å‡ºçš„è®­ç»ƒæ‰¹é‡
            "val_batch_size": prompt_mini_bsz,  # ğŸ”¥ éªŒè¯ç”¨å°æ‰¹é‡
            "max_prompt_length": kwargs.get('max_prompt_length', 1024),
            "max_response_length": kwargs.get('max_response_length', 512),
            "filter_overlong_prompts": True,
            "truncation": "error",
            "reward_fn_key": "data_source",
            "shuffle": True
        },
        
        # ğŸ”¥ ç›´è¿æ¨¡å¼ Actor é…ç½® - çœŸæ­£åŠ è½½æ¨¡å‹æƒé‡
        "actor_rollout_ref": {
            "model": {
                "path": model_path,
                "tokenizer_only": False,  # ğŸ”¥ å…³é”®ï¼šåŠ è½½å®Œæ•´æ¨¡å‹ï¼Œä¸åªæ˜¯tokenizer
                "use_remove_padding": True,
                "enable_gradient_checkpointing": True,
                "model_dtype": "bf16"
            },
            "actor": {
                "strategy": "fsdp",  # ğŸ”¥ ä½¿ç”¨ FSDP è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
                "optim": {
                    "lr": kwargs.get('learning_rate', 1e-6),
                    "eps": 1e-8,
                    "weight_decay": 0.01
                },
                # ğŸ”¥ CRITICAL: ä¿®å¤æ‰¹é‡å¤§å°é…ç½®
                "ppo_mini_batch_size": prompt_mini_bsz,  # 96
                "ppo_micro_batch_size": None,  # åºŸå¼ƒå­—æ®µ
                "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 2 (å¿…é¡»>0)
                "use_kl_loss": True,
                "kl_loss_coef": 0.001,
                "kl_loss_type": "low_var_kl",
                "entropy_coeff": 0.0,
                "entropy_from_logits_with_chunking": False,
                "clip_ratio": 0.2,
                "clip_ratio_low": 0.2,
                "clip_ratio_high": 0.2,

                "ppo_epochs": 1,
                "loss_agg_mode": "token-mean",
                "use_dynamic_bsz": True,
                "ulysses_sequence_parallel_size": 1,  # ç¦ç”¨åºåˆ—å¹¶è¡Œ
                "ppo_max_token_len_per_gpu": kwargs.get('max_model_len', 4096),

                # ğŸ”¥ æ·»åŠ å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
                "use_remove_padding": True,
                "use_fused_kernels": False,
                "use_torch_compile": False,
                "entropy_checkpointing": False,
                "grad_clip": 1.0,
                "shuffle": False,
                "data_loader_seed": None,
                # ğŸ”¥ æ·»åŠ policy_lossé…ç½®
                "policy_loss": {
                    "loss_mode": "vanilla"
                },
                "fsdp_config": {
                    "fsdp_size": gpus,
                    "param_offload": True,
                    "optimizer_offload": True,
                    "forward_prefetch": False,  # ğŸ”¥ ç¦ç”¨å‰å‘é¢„å–ä»¥èŠ‚çœå†…å­˜
                    "wrap_policy": {
                        "min_num_params": 0  # ğŸ”¥ æ›´å°çš„åŒ…è£…ç­–ç•¥
                    }
                },
                "checkpoint": {
                    "save_contents": ["model", "optimizer", "extra"],
                    "load_contents": ["model", "optimizer", "extra"]
                }
            },
            "rollout": {
                "name": "vllm",
                "mode": "sync",  # ğŸ”¥ CRITICAL: å¿…éœ€å­—æ®µ
                "n": rollout_n,  # 4
                "max_new_tokens": kwargs.get('max_new_tokens', 512),
                "load_format": "auto",
                "dtype": "bfloat16",
                "prompt_length": kwargs.get('max_prompt_length', 1024),
                "response_length": kwargs.get('max_new_tokens', 512),
                "max_model_len": kwargs.get('max_model_len', 4096),
                "enforce_eager": True,
                "enable_prefix_caching": False,
                "disable_log_stats": False,
                "enable_chunked_prefill": False,
                "disable_custom_all_reduce": True,
                "gpu_memory_utilization": 0.4,  # ğŸ”¥ é™ä½åˆ°0.4ä»¥é¿å…OOM
                "max_num_batched_tokens": 4096,  # ğŸ”¥ å‡å°‘æ‰¹é‡tokenæ•°é‡
                "seed": 0,
                "log_prob_use_dynamic_bsz": True,
                "ulysses_sequence_parallel_size": 1,  # ä¸ actor ä¿æŒä¸€è‡´

                "tensor_model_parallel_size": 1,
                "temperature": kwargs.get('temperature', 0.8),
                "top_p": kwargs.get('top_p', 0.9),
                "top_k": kwargs.get('top_k', 30),
                "log_prob_micro_batch_size": None,
                # ğŸ”¥ CRITICAL: refæ¨¡å‹ä¹Ÿéœ€è¦æ­£ç¡®çš„å¾®æ‰¹é‡å¤§å°

                "log_prob_max_token_len_per_gpu": kwargs.get('max_model_len', 24000),
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 1
                "val_kwargs": {
                    "do_sample": True,
                    "temperature": kwargs.get('temperature', 0.8),
                    "top_p": kwargs.get('top_p', 0.9),
                    "top_k": kwargs.get('top_k', 30),
                    "max_new_tokens": kwargs.get('max_new_tokens', 512),
                    "n": 1  # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„éªŒè¯æ—¶é‡‡æ ·æ•°é‡
                },
                # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„rolloutå­—æ®µ (ç›´è¿æ¨¡å¼)
                "calculate_log_probs": False,  # ç”¨äºè°ƒè¯•çš„rolloutæ¦‚ç‡è®°å½•
                "free_cache_engine": True,  # ç”Ÿæˆåé‡Šæ”¾KVç¼“å­˜å¼•æ“
                # ğŸ”¥ CRITICAL: å¿…éœ€çš„ multi_turn é…ç½®
                "multi_turn": {
                    "enable": False,
                    "max_turns": None,
                    "tool_config_path": None,
                    "completion_callback": None,
                    "use_inference_chat_template": False,
                    "enable_tokenization_sanity_check": True,
                    "format": "hermes"
                }
            },
            # ğŸ”¥ CRITICAL: ä¿®å¤ ref é…ç½®ï¼Œæ·»åŠ æ‰€æœ‰å¿…éœ€å­—æ®µ
            "ref": {
                "log_prob_micro_batch_size": None,
                # ğŸ”¥ CRITICAL: refæ¨¡å‹å¿…é¡»æœ‰æ­£ç¡®çš„å¾®æ‰¹é‡å¤§å° (ä¸èƒ½ä¸º0!)
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 1
                # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„ ulysses_sequence_parallel_size å­—æ®µ
                "ulysses_sequence_parallel_size": 1,  # ä¸ actor ä¿æŒä¸€è‡´
                "log_prob_use_dynamic_bsz": True,
                "log_prob_max_token_len_per_gpu": kwargs.get('max_model_len', 4096),
                # ğŸ”¥ æ·»åŠ  DataParallelPPOActor éœ€è¦çš„å…¶ä»–å­—æ®µ
                "use_remove_padding": True,  # ä¸ actor ä¿æŒä¸€è‡´
                "use_fused_kernels": False,  # ç¦ç”¨èåˆå†…æ ¸
                "entropy_from_logits_with_chunking": False,
                "use_torch_compile": False,
                "entropy_checkpointing": False,
                "grad_clip": 1.0,  # æ¢¯åº¦è£å‰ªï¼Œå³ä½¿refä¸ä¼˜åŒ–ä¹Ÿéœ€è¦
                "fsdp_config": {
                    "fsdp_size": gpus,
                    "param_offload": True,
                    "forward_prefetch": False,  # ğŸ”¥ ç¦ç”¨å‰å‘é¢„å–ä»¥èŠ‚çœå†…å­˜
                    "wrap_policy": {
                        "min_num_params": 0
                    }
                }
            },
            "hybrid_engine": True
        },
        
        # ğŸ”¥ CRITICAL: Critic é…ç½®ä¹Ÿéœ€è¦æ­£ç¡®çš„å¾®æ‰¹é‡å¤§å°
        "critic": {
            "strategy": "fsdp",
            "ppo_mini_batch_size": prompt_mini_bsz,  # 48
            "ppo_micro_batch_size": None,  # åºŸå¼ƒå­—æ®µ
            "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 1 (å¿…é¡»>0)
            "use_dynamic_bsz": True,
            "fsdp_config": {
                "fsdp_size": gpus,
                "param_offload": True,
                "optimizer_offload": True,
                "forward_prefetch": False,
                "wrap_policy": {
                    "min_num_params": 0
                }
            }
        },
        
        # å¥–åŠ±æ¨¡å‹é…ç½®
        "reward_model": {
            "enable": False,
            "launch_reward_fn_async": False
        },
        
        # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        "custom_reward_function": {
            "path": reward_file_path,
            "name": "compute_score"
        },
        
        # è®­ç»ƒå™¨é…ç½®
        "trainer": {
            "total_epochs": kwargs.get('epochs', 10),
            "total_training_steps": None,
            "project_name": "llm_sr_grpo_direct",
            "experiment_name": kwargs.get('experiment_name', 'direct_weight_tuning_fixed'),
            "logger": ["console"],
            "n_gpus_per_node": gpus,
            "nnodes": 1,
            "save_freq": kwargs.get('save_freq', 2),
            "test_freq": kwargs.get('test_freq', 5),
            "val_before_train": True,
            "default_local_dir": output_dir,
            "device": "cuda",
            "resume_mode": "disable",
            "default_hdfs_dir": None,

            # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„é…ç½®é¡¹ä»¥é¿å… ConfigAttributeError
            "log_val_generations": kwargs.get('log_val_generations', 10),  # éªŒè¯æ—¶è®°å½•çš„ç”Ÿæˆæ ·æœ¬æ•°
            "log_train_generations": kwargs.get('log_train_generations', 5),  # è®­ç»ƒæ—¶è®°å½•çš„ç”Ÿæˆæ ·æœ¬æ•°
            "profile_steps": None,
            "balance_batch": None,
            "critic_warmup": 0, # ğŸ”¥ FIX: Add missing critic_warmup key

            "log_prob_max_token_len_per_gpu": kwargs.get('max_model_len', 4096)
        },
        
        # Ray åˆå§‹åŒ–
        "ray_init": {
            "num_cpus": None
        }
    }
    
    return OmegaConf.create(config)


def create_grpo_config(
    model_path: str,
    dataset_path: str, 
    reward_file_path: str,
    output_dir: str,
    **kwargs
) -> DictConfig:
    """
    åˆ›å»º VERL GRPO é…ç½® - å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œå®é™…è°ƒç”¨ç›´è¿æ¨¡å¼
    """
    print("âš ï¸  ä½¿ç”¨ç›´è¿æ¨¡å¼é…ç½® (çœŸæ­£å¾®è°ƒæƒé‡)")
    return create_grpo_config_direct(
        model_path=model_path,
        dataset_path=dataset_path,
        reward_file_path=reward_file_path,
        output_dir=output_dir,
        **kwargs
    )


def create_grpo_config_http(
    http_url: str,
    tokenizer_path: str,
    dataset_path: str, 
    reward_file_path: str,
    output_dir: str,
    **kwargs
) -> DictConfig:
    """
    åˆ›å»º HTTP æ¨¡å¼çš„ VERL GRPO é…ç½® - ä¸æ›´æ–°æƒé‡ï¼Œä»…ç”¨äºæ¨ç†
    
    âš ï¸  æ³¨æ„ï¼šHTTP æ¨¡å¼ä¸‹æƒé‡ä¸ä¼šæ›´æ–°ï¼
    """
    
    # ğŸ”¥ æ‰¹é‡å¤§å°é…ç½® - å‚ç…§VERLå®˜æ–¹ç¤ºä¾‹ (HTTPæ¨¡å¼ç”¨è¾ƒå°é…ç½®)
    gpus = kwargs.get('gpus', 2)
    micro_batch_size_per_gpu = 2  # æ¯GPUå¾®æ‰¹é‡å¤§å° (å¿…é¡»>0)
    rollout_n = kwargs.get('rollout_n', 4)  # å“åº”æ•°é‡
    
    # è®¡ç®—å„çº§æ‰¹é‡å¤§å° (å‚ç…§VERLå®˜æ–¹å…¬å¼)
    traj_micro_bsz = micro_batch_size_per_gpu * gpus          # 2 * 2 = 4
    traj_mini_bsz = traj_micro_bsz * 2                        # 4 * 2 = 8
    prompt_mini_bsz = traj_mini_bsz * rollout_n               # 8 * 4 = 32
    prompt_bsz = prompt_mini_bsz * 2                          # 32 * 2 = 64
    
    # HTTP æ¨¡å¼é…ç½®
    config = {
        # ç®—æ³•é…ç½®
        "algorithm": {
            "adv_estimator": "grpo",
            "gamma": 1.0,
            "lam": 1.0,
            "norm_adv_by_std_in_grpo": True,
            "use_kl_in_reward": False,
            "kl_ctrl": {
                "type": "fixed",
                "kl_coef": 0.001,
                "horizon": 10000,
                "target_kl": 0.1
            }
        },
        
        # æ•°æ®é…ç½®
        "data": {
            "train_files": [dataset_path],
            "val_files": [dataset_path],
            "train_batch_size": prompt_bsz,  # 64
            "val_batch_size": prompt_mini_bsz,  # 32
            "max_prompt_length": kwargs.get('max_prompt_length', 1024),
            "max_response_length": kwargs.get('max_response_length', 512),
            "filter_overlong_prompts": True,
            "truncation": "error",
            "reward_fn_key": "data_source",
            "shuffle": True
        },
        
        # ğŸ”¥ HTTP æ¨¡å¼ Actor é…ç½® - åªåŠ è½½ tokenizerï¼Œä¸æ›´æ–°æƒé‡
        "actor_rollout_ref": {
            "model": {
                "path": tokenizer_path,
                "tokenizer_only": True  # ğŸ”¥ å…³é”®ï¼šåªåŠ è½½tokenizerï¼Œä¸åŠ è½½æ¨¡å‹æƒé‡
            },
            "actor": {
                "strategy": "fsdp",
                "optim": {
                    "lr": kwargs.get('learning_rate', 1e-6)
                },
                "ppo_mini_batch_size": prompt_mini_bsz,  # 32
                "ppo_micro_batch_size": None,  # åºŸå¼ƒå­—æ®µ
                "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 2
                "use_kl_loss": False,
                "entropy_coeff": 0,
                "entropy_from_logits_with_chunking": False,
                "clip_ratio": 0.2,
                "clip_ratio_low": 0.2,
                "clip_ratio_high": 0.2,

                "ppo_epochs": 1,
                "loss_agg_mode": "token-mean",
                "use_dynamic_bsz": True,
                "ulysses_sequence_parallel_size": 1,  # ç¦ç”¨åºåˆ—å¹¶è¡Œ
                "fsdp_config": {
                    "fsdp_size": gpus,
                    "param_offload": True,
                    "optimizer_offload": True,
                    "forward_prefetch": False,
                    "wrap_policy": {
                        "min_num_params": 0
                    }
                },
                "checkpoint": {
                    "save_contents": [],  # HTTPæ¨¡å¼ä¸ä¿å­˜æƒé‡
                    "load_contents": []   # HTTPæ¨¡å¼ä¸åŠ è½½æƒé‡
                }
            },
            "rollout": {
                "name": "vllm",
                "mode": "sync",  # ğŸ”¥ CRITICAL: åŒæ­¥æ¨¡å¼ï¼Œå¿…éœ€å­—æ®µ
                "http_url": http_url,
                "n": rollout_n,  # 4
                "max_new_tokens": kwargs.get('max_new_tokens', 512),
                "load_format": "auto",
                "dtype": "bfloat16",
                "log_prob_use_dynamic_bsz": True,

                "prompt_length": kwargs.get('max_prompt_length', 1024),
                "response_length": kwargs.get('max_new_tokens', 512),
                "max_model_len": kwargs.get('max_model_len', 4096),
                "enforce_eager": True,
                "free_cache_engine": False,
                "enable_prefix_caching": False,
                "disable_log_stats": False,
                "enable_chunked_prefill": False,
                "disable_custom_all_reduce": True,
                "gpu_memory_utilization": 0.85,
                "max_num_batched_tokens": 8192,
                "seed": 0,
                "tensor_model_parallel_size": 1,
                "temperature": kwargs.get('temperature', 0.8),
                "top_p": kwargs.get('top_p', 0.9),
                "top_k": kwargs.get('top_k', 30),
                "timeout": kwargs.get('timeout', 60),
                "repeat_prompt": 1,
                "log_prob_micro_batch_size": None,
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 2
                "log_prob_max_token_len_per_gpu": kwargs.get('max_model_len', 24000),
                # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„éªŒè¯é…ç½®

                "val_kwargs": {
                    "do_sample": True,
                    "n": rollout_n,
                    "max_new_tokens": kwargs.get('max_new_tokens', 512),
                    "temperature": kwargs.get('temperature', 0.8),
                    "top_p": kwargs.get('top_p', 0.9),
                    "top_k": kwargs.get('top_k', 30)
                },
                # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„rolloutå­—æ®µ (HTTPæ¨¡å¼)
                "calculate_log_probs": False,  # ç”¨äºè°ƒè¯•çš„rolloutæ¦‚ç‡è®°å½•
                "free_cache_engine": True,  # ç”Ÿæˆåé‡Šæ”¾KVç¼“å­˜å¼•æ“
                # ğŸ”¥ CRITICAL: æ·»åŠ ç¼ºå¤±çš„ multi_turn é…ç½®
                "multi_turn": {
                    "enable": False,
                    "max_turns": None,
                    "tool_config_path": None,
                    "completion_callback": None,
                    "use_inference_chat_template": False,
                    "enable_tokenization_sanity_check": True,
                    "format": "hermes"
                }
            },
            # ğŸ”¥ CRITICAL: ä¿®å¤HTTPæ¨¡å¼çš„refé…ç½®
            "ref": {
                "disable": True,
                "log_prob_micro_batch_size": None,
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 2
                # ğŸ”¥ æ·»åŠ å¿…éœ€çš„é…ç½®å­—æ®µ
                "ulysses_sequence_parallel_size": 1,
                "use_remove_padding": False,  # HTTPæ¨¡å¼ç¦ç”¨
                "use_fused_kernels": False,
                "entropy_from_logits_with_chunking": False,
                "use_torch_compile": False,  # HTTPæ¨¡å¼ç¦ç”¨
                "entropy_checkpointing": False,
                "grad_clip": 1.0,
                "fsdp_config": {
                    "fsdp_size": gpus,
                    "param_offload": True,
                    "forward_prefetch": False,
                    "wrap_policy": {
                        "min_num_params": 0
                    }
                }
            },
            "hybrid_engine": True
        },
        
        # Critic é…ç½®
        "critic": {
            "strategy": "fsdp",
            "ppo_mini_batch_size": prompt_mini_bsz,  # 32
            "ppo_micro_batch_size": None,  # åºŸå¼ƒå­—æ®µ
            "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,  # 2
            "use_dynamic_bsz": True
        },
        
        # å¥–åŠ±æ¨¡å‹é…ç½®
        "reward_model": {
            "enable": False,
            "launch_reward_fn_async": False
        },
        
        # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        "custom_reward_function": {
            "path": reward_file_path,
            "name": "compute_score"
        },
        
        # è®­ç»ƒå™¨é…ç½®
        "trainer": {
            "total_epochs": kwargs.get('epochs', 10),
            "total_training_steps": None,
            "project_name": "llm_sr_grpo_http",
            "experiment_name": kwargs.get('experiment_name', 'http_mode_no_weight_update'),
            "logger": ["console"],
            "n_gpus_per_node": gpus,
            "nnodes": 1,
            "save_freq": kwargs.get('save_freq', -1),
            "test_freq": kwargs.get('test_freq', 5),
            "val_before_train": True,
            "default_local_dir": output_dir,
            "device": "cuda",
            "critic_warmup": 0, # ğŸ”¥ FIX: Add missing critic_warmup key
            "default_hdfs_dir": None,

            "resume_mode": "disable"
        },
        
        # Ray åˆå§‹åŒ–
        "ray_init": {
            "num_cpus": None
        }
    }
    
    return OmegaConf.create(config)


def train_llmsr_grpo_direct(
    template_path: str,
    data_path: str, 
    model_path: str = "Qwen/Qwen2.5-1.5B-Instruct",
    output_dir: str = "./llmsr_grpo_outputs",
    **kwargs
):
    """
    ğŸ”¥ ç›´è¿æ¨¡å¼ LLM-SR GRPO è®­ç»ƒ - çœŸæ­£å¾®è°ƒ LLM æƒé‡
    
    Args:
        template_path: LLM-SR è§„èŒƒæ¨¡æ¿è·¯å¾„
        data_path: è®­ç»ƒæ•°æ® CSV è·¯å¾„
        model_path: æ¨¡å‹è·¯å¾„æˆ–åç§°
        output_dir: è¾“å‡ºç›®å½•
        **kwargs: é¢å¤–è®­ç»ƒé…ç½®
    """
    
    print("ğŸ”¥ å¯åŠ¨ LLM-SR GRPO ç›´è¿æ¨¡å¼è®­ç»ƒ (çœŸæ­£å¾®è°ƒæƒé‡)")
    print(f"ğŸ“‹ æ¨¡æ¿: {template_path}")
    print(f"ğŸ“Š æ•°æ®: {data_path}")
    print(f"ğŸ¤– æ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡º: {output_dir}")
    print(f"ğŸ”§ ç›´è¿æ¨¡å¼: Actor è¿›ç¨‹ç›´æ¥åŠ è½½æ¨¡å‹ï¼Œé€šè¿‡ FSDP æ›´æ–°æƒé‡")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ğŸ”¥ æ­¥éª¤ 1: æ£€æŸ¥é¢„ç”Ÿæˆçš„VERLæ•°æ®é›†
    print("\nğŸ“ æ­¥éª¤ 1: æ£€æŸ¥ VERL æ•°æ®é›†...")
    
    # ä»data_pathæå–problem_name
    problem_name = None
    if 'oscillator1' in data_path:
        problem_name = 'oscillator1'
    elif 'oscillator2' in data_path:
        problem_name = 'oscillator2'
    elif 'bactgrow' in data_path:
        problem_name = 'bactgrow'
    elif 'stressstrain' in data_path:
        problem_name = 'stressstrain'
    
    # æ£€æŸ¥é¢„ç”Ÿæˆçš„æ•°æ®é›†
    verl_dataset_path = None
    if problem_name:
        potential_path = f"./verl_datasets/{problem_name}_train_verl.parquet"
        if os.path.exists(potential_path):
            verl_dataset_path = potential_path
            print(f"âœ… æ‰¾åˆ°é¢„ç”Ÿæˆçš„VERLæ•°æ®é›†: {verl_dataset_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°é¢„ç”Ÿæˆçš„VERLæ•°æ®é›†: {potential_path}")
    
    # å¦‚æœæ²¡æœ‰é¢„ç”Ÿæˆçš„æ•°æ®é›†ï¼Œåˆ™åˆ›å»ºæ–°çš„
    if not verl_dataset_path:
        print("ğŸ“ åˆ›å»ºæ–°çš„ VERL æ•°æ®é›†...")
        dataset_path = create_llmsr_dataset(template_path, data_path, output_dir)
    else:
        dataset_path = verl_dataset_path
        print(f"ğŸ“Š ä½¿ç”¨é¢„ç”Ÿæˆçš„æ•°æ®é›†: {dataset_path}")
    
    # æ­¥éª¤ 2: åˆ›å»ºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
    print("\nğŸ¯ æ­¥éª¤ 2: åˆ›å»ºå¥–åŠ±å‡½æ•°...")
    reward_file_path = create_llmsr_reward_file(template_path, data_path, output_dir)
    
    # æ­¥éª¤ 3: åˆ›å»ºç›´è¿æ¨¡å¼ GRPO é…ç½®
    print("\nâš™ï¸ æ­¥éª¤ 3: åˆ›å»ºç›´è¿æ¨¡å¼ GRPO é…ç½®...")
    config = create_grpo_config_direct(
        model_path=model_path,
        dataset_path=dataset_path,
        reward_file_path=reward_file_path,
        output_dir=output_dir,
        **kwargs
    )
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(output_dir, "grpo_config_direct.yaml")
    OmegaConf.save(config, config_path)
    print(f"ğŸ’¾ ä¿å­˜ç›´è¿æ¨¡å¼é…ç½®: {config_path}")
    
    # æ­¥éª¤ 4: å¯åŠ¨ GRPO è®­ç»ƒ
    print("\nğŸ”¥ æ­¥éª¤ 4: å¯åŠ¨ GRPO ç›´è¿æ¨¡å¼è®­ç»ƒ...")
    print("âš¡ æ¨¡å‹æƒé‡å°†é€šè¿‡ FSDP è¿›è¡ŒçœŸæ­£çš„å¾®è°ƒæ›´æ–°")
    try:
        run_ppo(config)
        print("âœ… ç›´è¿æ¨¡å¼è®­ç»ƒå®Œæˆï¼Œæƒé‡å·²æ›´æ–°ï¼")
    except Exception as e:
        print(f"âŒ ç›´è¿æ¨¡å¼è®­ç»ƒå¤±è´¥: {e}")
        raise


def train_llmsr_grpo(
    template_path: str,
    data_path: str, 
    model_path: str = "Qwen/Qwen2.5-1.5B-Instruct",
    output_dir: str = "./llmsr_grpo_outputs",
    **kwargs
):
    """
    ä¸»è¦å…¥å£ç‚¹ - é»˜è®¤ä½¿ç”¨ç›´è¿æ¨¡å¼è¿›è¡Œæƒé‡å¾®è°ƒ
    """
    print("ğŸ”„ é‡å®šå‘åˆ°ç›´è¿æ¨¡å¼è®­ç»ƒ...")
    return train_llmsr_grpo_direct(
        template_path=template_path,
        data_path=data_path,
        model_path=model_path,
        output_dir=output_dir,
        **kwargs
    )


def train_llmsr_grpo_http(
    template_path: str,
    data_path: str, 
    http_url: str = "http://localhost:5000",
    tokenizer_path: str = "mistralai/Mixtral-8x7B-Instruct-v0.1",
    output_dir: str = "./llmsr_grpo_outputs",
    **kwargs
):
    """
    HTTP æ¨¡å¼ LLM-SR GRPO è®­ç»ƒ - ä¸æ›´æ–°æƒé‡ï¼Œä»…ç”¨äºæ¨ç†
    
    âš ï¸  é‡è¦ï¼šHTTP æ¨¡å¼ä¸‹æƒé‡ä¸ä¼šæ›´æ–°ï¼å¦‚éœ€å¾®è°ƒæƒé‡ï¼Œè¯·ä½¿ç”¨ train_llmsr_grpo_direct()
    """
    
    print("ğŸŒ å¯åŠ¨ LLM-SR GRPO HTTP æ¨¡å¼è®­ç»ƒ (ä¸æ›´æ–°æƒé‡)")
    print(f"ğŸ“‹ æ¨¡æ¿: {template_path}")
    print(f"ğŸ“Š æ•°æ®: {data_path}")
    print(f"ğŸŒ HTTP æœåŠ¡: {http_url}")
    print(f"ğŸ”¤ åˆ†è¯å™¨: {tokenizer_path}")
    print(f"ğŸ“ è¾“å‡º: {output_dir}")
    print(f"âš ï¸  æ³¨æ„: HTTP æ¨¡å¼ä¸‹æƒé‡ä¸ä¼šæ›´æ–°ï¼")
    
    # æµ‹è¯• HTTP æœåŠ¡è¿æ¥
    print(f"\nğŸ” æµ‹è¯• HTTP æœåŠ¡è¿æ¥...")
    try:
        import requests
        response = requests.get(http_url, timeout=5)
        if response.status_code == 200:
            print(f"âœ… HTTP æœåŠ¡å“åº”æ­£å¸¸")
        else:
            print(f"âš ï¸ HTTP æœåŠ¡è¿”å›çŠ¶æ€ç  {response.status_code}")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° HTTP æœåŠ¡: {e}")
        print("ğŸ”§ è¯·ç¡®ä¿ LLM å¼•æ“æ­£åœ¨è¿è¡Œ:")
        print("   ./run_llmsr_engine.sh")
        raise
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ­¥éª¤ 1: åˆ›å»º VERL å…¼å®¹æ•°æ®é›†
    print("\nğŸ“ æ­¥éª¤ 1: åˆ›å»º VERL æ•°æ®é›†...")
    dataset_path = create_llmsr_dataset(template_path, data_path, output_dir)
    
    # æ­¥éª¤ 2: åˆ›å»ºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
    print("\nğŸ¯ æ­¥éª¤ 2: åˆ›å»ºå¥–åŠ±å‡½æ•°...")
    reward_file_path = create_llmsr_reward_file(template_path, data_path, output_dir)
    
    # æ­¥éª¤ 3: åˆ›å»º HTTP æ¨¡å¼ GRPO é…ç½®
    print("\nâš™ï¸ æ­¥éª¤ 3: åˆ›å»º HTTP æ¨¡å¼ GRPO é…ç½®...")
    config = create_grpo_config_http(
        http_url=http_url,
        tokenizer_path=tokenizer_path,
        dataset_path=dataset_path,
        reward_file_path=reward_file_path,
        output_dir=output_dir,
        **kwargs
    )
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(output_dir, "grpo_config_http.yaml")
    OmegaConf.save(config, config_path)
    print(f"ğŸ’¾ ä¿å­˜ HTTP æ¨¡å¼é…ç½®: {config_path}")
    
    # æ­¥éª¤ 4: å¯åŠ¨ GRPO è®­ç»ƒ
    print("\nğŸ”¥ æ­¥éª¤ 4: å¯åŠ¨ GRPO HTTP æ¨¡å¼è®­ç»ƒ...")
    print("âš ï¸  æ³¨æ„: æƒé‡ä¸ä¼šæ›´æ–°ï¼Œä»…è¿›è¡Œç­–ç•¥ä¼˜åŒ–")
    try:
        run_ppo(config)
        print("âœ… HTTP æ¨¡å¼è®­ç»ƒå®Œæˆ (æƒé‡æœªæ›´æ–°)")
    except Exception as e:
        print(f"âŒ HTTP æ¨¡å¼è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # ğŸ”¥ é»˜è®¤ä½¿ç”¨ç›´è¿æ¨¡å¼è¿›è¡Œæƒé‡å¾®è°ƒ
    train_llmsr_grpo_direct(
        template_path="./specs/specification_oscillator1_numpy.txt",
        data_path="./data/oscillator1/train.csv",
        model_path="Qwen/Qwen2.5-Coder-7B-Instruct",
        epochs=5,
        batch_size=32,
        learning_rate=1e-6,
        gpus=6
    ) 