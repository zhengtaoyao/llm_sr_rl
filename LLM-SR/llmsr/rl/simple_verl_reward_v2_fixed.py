#!/usr/bin/env python3
"""
ğŸ”¥ ä¿®å¤ç‰ˆVERLå¥–åŠ±å‡½æ•° V2 - é‡‡ç”¨æ— RLç‰ˆæœ¬çš„æ­£ç¡®æ–¹æ³• + å¤šæˆåˆ†å¥–åŠ±

å…³é”®æ”¹è¿›ï¼š
1. ä¸å†æå–æ•°å­¦è¡¨è¾¾å¼å­—ç¬¦ä¸²
2. ç›´æ¥æ‰§è¡ŒLLMç”Ÿæˆçš„å®Œæ•´Pythonå‡½æ•°
3. ä¿æŒV2ç‰ˆæœ¬çš„å¤šæˆåˆ†å¥–åŠ±è®¡ç®—
4. æ¨¡ä»¿æ— RLç‰ˆæœ¬çš„å¤„ç†æµç¨‹
"""

import math
import re
from typing import Any, Dict, List, Tuple
import numpy as np
import os, json, time
import ast
import pandas as pd
from pathlib import Path


def compute_score(
    data_sources: List[Any] | None = None,
    solution_strs: List[str] | None = None,
    ground_truths: List[Any] | None = None,
    extra_infos: List[Dict[str, Any]] | None = None,
    *,
    grid_train_data: bool = False,
    template_path: str | None = None,
    data_path: str | None = None,
    memory_dir: str | None = None,
    lambda_nmse: float = 3.0,
    lambda_simp: float = 0.1,
    w_fit: float = 0.6,
    w_simp: float = 0.2,
    w_phys: float = 0.15,
    w_proc: float = 0.05,
    groupwise_rank_norm: bool = True,
    # ğŸ”¥ æ–°å¢é•¿åº¦æƒ©ç½šå’Œè§£æå¥–åŠ±å‚æ•°
    length_penalty_alpha: float = 0.03,  # é•¿åº¦æƒ©ç½šç³»æ•°ï¼Œå»ºè®®0.02-0.05
    parse_bonus: float = 0.1,            # è§£ææˆåŠŸå¥–åŠ±
    invalid_penalty: float = -0.5,       # æ— æ•ˆæ ·æœ¬æƒ©ç½š
    # ğŸ”¥ ç‰©ç†ä¸€è‡´æ€§å¥–åŠ±å¼€å…³ï¼ˆé»˜è®¤å…³é—­ï¼‰
    enable_physics_reward: bool = False,  # æ˜¯å¦å¯ç”¨ç‰©ç†ä¸€è‡´æ€§å¥–åŠ±
    **kwargs,
):
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ FIXED V2 REWARD FUNCTION CALLED! ğŸ”¥ğŸ”¥ğŸ”¥")
    print(f"ğŸ”§ V2ä¿®å¤ç‰ˆå‚æ•°ç±»å‹: data_sources={type(data_sources)}, solution_strs={type(solution_strs)}, ground_truths={type(ground_truths)}, extra_infos={type(extra_infos)}")
    print(f"ğŸ”§ V2 Solution strings count: {len(solution_strs) if solution_strs else 0}")
    if solution_strs and len(solution_strs) > 0:
        print(f"ğŸ”§ V2 First solution preview: {solution_strs[0][:200] if solution_strs[0] else 'None'}...")
    print(f"ğŸ”§ V2 LLMSR_OUTPUT_DIR env: {os.environ.get('LLMSR_OUTPUT_DIR', 'NOT_SET')}")
    
    # è¾“å…¥å…œåº•
    solution_strs = solution_strs or []
    extra_infos = extra_infos or [{} for _ in range(len(solution_strs))]
    if len(solution_strs) == 0:
        return []

    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰
    inputs, outputs, var_names = _load_training_data_from_path(data_path)
    if inputs is None:
        # è¿”å›æƒ©ç½š
        return [float(-1.0)] * len(solution_strs)

    # è®¡ç®—å„é¡¹ reward
    out_dir = os.environ.get("LLMSR_OUTPUT_DIR")
    jsonl_path = None
    if out_dir:
        try:
            os.makedirs(out_dir, exist_ok=True)
            jsonl_path = os.path.join(out_dir, "sample.jsonl")
        except Exception:
            jsonl_path = None

    rewards: List[float] = []
    for i, code in enumerate(solution_strs):
        base_impl = None
        if i < len(extra_infos) and isinstance(extra_infos[i], dict):
            base_impl = extra_infos[i].get("base_impl")

        # æ”¯æŒ EDIT DSLï¼šè‹¥åŒ…å« EDIT æŒ‡ä»¤åˆ™åŸºäº base_impl ç”Ÿæˆè¡¨è¾¾å¼
        edit_mode = False
        
        # ğŸ”¥ ä½¿ç”¨æ–°çš„æ–¹æ³•ï¼šç›´æ¥æ‰§è¡ŒPythonå‡½æ•°
        base_reward, execution_success, mse, complexity, params_used = evaluate_single_solution_v2_fixed(
            code, inputs, outputs, var_names, lambda_nmse, lambda_simp, w_fit, w_simp, w_phys, w_proc, enable_physics_reward
        )
        
        # ğŸ”¥ è®¡ç®—é•¿åº¦æƒ©ç½šï¼šreward := base_reward - Î±Â·(len_tokens/1k)
        len_tokens = _estimate_token_length(code)
        length_penalty = length_penalty_alpha * (len_tokens / 1000.0)
        
        # ğŸ”¥ è§£æå¥–åŠ±å’Œæ— æ•ˆæƒ©ç½š
        if execution_success:
            parse_reward = parse_bonus  # è§£ææˆåŠŸå¥–åŠ±
        else:
            parse_reward = invalid_penalty  # æ— æ•ˆæ ·æœ¬æƒ©ç½š
        
        # ğŸ”¥ æœ€ç»ˆå¥–åŠ± = åŸºç¡€å¥–åŠ± - é•¿åº¦æƒ©ç½š + è§£æå¥–åŠ±/æƒ©ç½š
        final_reward = base_reward - length_penalty + parse_reward
        
        rewards.append(float(final_reward))

        # è®°å½•æ ·æœ¬
        if jsonl_path:
            try:
                rec = {
                    "timestamp": time.time(),
                    "expr": "ç›´æ¥æ‰§è¡ŒPythonå‡½æ•°",
                    "params": params_used.tolist() if params_used is not None else None,
                    "reward": float(final_reward),
                    "base_reward": float(base_reward),
                    "length_penalty": float(length_penalty),
                    "parse_reward": float(parse_reward),
                    "len_tokens": int(len_tokens),
                    "nmse": float(mse) if mse is not None else None,
                    "complexity": float(complexity) if complexity is not None else None,
                    "r_fit": None,
                    "r_simp": None,
                    "r_phys": None,
                    "r_proc": None,
                    "edit_mode": edit_mode,
                    "ast_ok": execution_success,
                    "data_path": data_path,
                }
                with open(jsonl_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            except Exception:
                pass

    # ç»„å†…æ’åå½’ä¸€ï¼ˆè‹¥ VERL æ‰¹æ¬¡æ¥è‡ªåŒä¸€æç¤ºç»„ï¼Œå¯é™ä½å°ºåº¦å™ªå£°ï¼‰
    if groupwise_rank_norm and len(rewards) >= 2:
        order = np.argsort(rewards)[::-1]
        ranks = np.empty_like(order)
        ranks[order] = np.arange(len(rewards))
        rewards = (1.0 - ranks / max(1, len(rewards) - 1)).astype(np.float32).tolist()

    # VERL å…¼å®¹ï¼šè¿”å› float æˆ– list
    if len(rewards) == 1:
        return float(rewards[0])
    return rewards


def evaluate_single_solution_v2_fixed(
    solution_str: str, 
    inputs: np.ndarray, 
    outputs: np.ndarray, 
    var_names: list,
    lambda_nmse: float = 3.0,
    lambda_simp: float = 0.1,
    w_fit: float = 0.6,
    w_simp: float = 0.2,
    w_phys: float = 0.15,
    w_proc: float = 0.05,
    enable_physics_reward: bool = False
):
    """
    ğŸ”¥ V2ä¿®å¤ç‰ˆï¼šä½¿ç”¨æ— RLç‰ˆæœ¬çš„æ–¹æ³•ç›´æ¥æ‰§è¡ŒPythonå‡½æ•° + å¤šæˆåˆ†å¥–åŠ±
    
    Returns:
        reward, execution_success, mse, complexity, params_used
    """
    
    try:
        # ğŸ”¥ æ­¥éª¤1ï¼šä»LLMè¾“å‡ºä¸­æå–å‡½æ•°ä½“
        function_body = extract_function_body_v2(solution_str)
        
        if not function_body:
            print(f"âŒ V2å‡½æ•°ä½“æå–å¤±è´¥")
            return -1.0, False, 1e6, 0.0, None
        
        print(f"âœ… V2æˆåŠŸæå–å‡½æ•°ä½“ï¼Œé•¿åº¦: {len(function_body)}")
        
        # ğŸ”¥ æ­¥éª¤2ï¼šæ„å»ºå®Œæ•´çš„å¯æ‰§è¡Œç¨‹åº
        program = build_executable_program_v2(function_body, var_names)
        
        # ğŸ”¥ æ­¥éª¤3ï¼šåœ¨å®‰å…¨ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºå¹¶è®¡ç®—MSE
        mse, params_used = execute_and_compute_mse_v2(program, inputs, outputs, var_names)
        
        if mse >= 1e6:
            return -1.0, False, mse, 0.0, params_used
        
        # è®¡ç®—NMSE
        var_y = float(np.var(outputs) + 1e-9)
        nmse = mse / var_y
        
        # è®¡ç®—å„é¡¹å¥–åŠ±
        r_fit = math.exp(-lambda_nmse * nmse)
        
        # ä¼°ç®—å¤æ‚åº¦ï¼ˆåŸºäºå‡½æ•°ä½“é•¿åº¦ï¼‰
        complexity = _estimate_complexity_from_body(function_body)
        r_simp = math.exp(-lambda_simp * complexity)
        
        # ç‰©ç†ä¸€è‡´æ€§ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰
        if enable_physics_reward:
            r_phys = _physical_consistency_v2(function_body, var_names, inputs, outputs)
        else:
            r_phys = 1.0  # é»˜è®¤ä¸æƒ©ç½š
            w_phys = 0.0  # æƒé‡è®¾ä¸º0ï¼Œä¸å½±å“æ€»å¥–åŠ±
        
        # è¿‡ç¨‹å¥–åŠ±ï¼ˆç®€åŒ–ç‰ˆï¼‰
        r_proc = 0.5 if mse < 1.0 else 0.0
        
        # ç»¼åˆå¥–åŠ±
        reward = w_fit * r_fit + w_simp * r_simp + w_phys * r_phys + w_proc * r_proc
        
        print(f"âœ… V2è®¡ç®—å®Œæˆ - MSE: {mse:.6f}, å¥–åŠ±: {reward:.6f}")
        
        return reward, True, mse, complexity, params_used
        
    except Exception as e:
        print(f"âŒ V2æ‰§è¡ŒPythonå‡½æ•°æ—¶å‡ºé”™: {e}")
        return -1.0, False, 1e6, 0.0, None


class _FunctionLineVisitorV2(ast.NodeVisitor):
    """ V2ç‰ˆæœ¬ï¼šVisitor that finds the last line number of a function with a given name."""

    def __init__(self, target_function_name: str) -> None:
        self._target_function_name: str = target_function_name
        self._function_end_line: int | None = None

    def visit_FunctionDef(self, node: Any) -> None: 
        """ Collect the end line number of the target function."""
        if node.name == self._target_function_name:
            self._function_end_line = node.end_lineno
        self.generic_visit(node)

    @property
    def function_end_line(self) -> int:
        """ Line number of the final line of function `target_function_name`."""
        assert self._function_end_line is not None 
        return self._function_end_line


def _trim_function_body_v2(generated_code: str) -> str:
    """ V2ç‰ˆæœ¬ï¼šExtract the body of the generated function, trimming anything after it.
    Please note that the indentation is REQUIRED !!!
    """
    if not generated_code:
        return ''

    code = f'def fake_function_header():\n{generated_code}'

    tree = None
    while tree is None:
        try:
            tree = ast.parse(code)
        
        except SyntaxError as e:
            if e.lineno is None: # Nothing could be saved when syntaxError
                return ''
            code = '\n'.join(code.splitlines()[:e.lineno - 1])

    if not code:
        return ''

    visitor = _FunctionLineVisitorV2('fake_function_header')
    visitor.visit(tree)
    body_lines = code.splitlines()[1:visitor.function_end_line]
    return '\n'.join(body_lines) + '\n\n'


def extract_function_body_v2(solution_str: str) -> str:
    """V2ç‰ˆæœ¬ï¼šä»LLMè¾“å‡ºä¸­æå–å‡½æ•°ä½“ï¼Œå®Œå…¨æ¨¡ä»¿sampler.pyçš„_extract_bodyå‡½æ•°"""
    if not solution_str or not isinstance(solution_str, str):
        return ""
    
    # å¤„ç†<think>æ ‡ç­¾
    if "</think>" in solution_str:
        parts = solution_str.split("</think>")
        if len(parts) > 1:
            solution_str = parts[-1].strip()
    
    # æŸ¥æ‰¾Pythonä»£ç å—
    import re
    code_block_patterns = [
        r'```python\s*\n(.*?)\n```',
        r'```\s*\n(.*?)\n```'
    ]
    
    for pattern in code_block_patterns:
        matches = re.findall(pattern, solution_str, re.DOTALL)
        if matches:
            solution_str = matches[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»£ç å—
            break
    
    # ğŸ”¥ å®Œå…¨æ¨¡ä»¿sampler.pyçš„_extract_bodyé€»è¾‘
    lines = solution_str.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    
    for lineno, line in enumerate(lines):
        # find the first 'def' program statement in the response
        if line[:3] == 'def':  # ğŸ”¥ ä½¿ç”¨æ— RLç‰ˆæœ¬çš„ç²¾ç¡®åŒ¹é…
            func_body_lineno = lineno
            find_def_declaration = True
            break
    
    if find_def_declaration:
        # ğŸ”¥ æ¨¡ä»¿æ— RLç‰ˆæœ¬çš„ç¼©è¿›å¤„ç†é€»è¾‘
        code = ''
        indent = '    '
        for line in lines[func_body_lineno + 1:]:
            if line[:4] != indent:
                line = indent + line
            code += line + '\n'
        
        # ğŸ”¥ ä½¿ç”¨æ— RLç‰ˆæœ¬çš„_trim_function_bodyç¡®ä¿è¯­æ³•æ­£ç¡®
        return _trim_function_body_v2(code)
    
    return solution_str  # ğŸ”¥ å¦‚æœæ²¡æ‰¾åˆ°defï¼Œè¿”å›åŸå§‹sampleï¼ˆæ— RLç‰ˆæœ¬çš„è¡Œä¸ºï¼‰


def build_executable_program_v2(function_body: str, var_names: list) -> str:
    """V2ç‰ˆæœ¬ï¼šæ„å»ºå®Œæ•´çš„å¯æ‰§è¡Œç¨‹åº"""
    
    # æ„å»ºå‡½æ•°ç­¾å
    params_str = ', '.join(var_names) + ', params'
    
    # æ„å»ºå®Œæ•´çš„ç¨‹åº
    program = f"""
import numpy as np
import math
from scipy.optimize import minimize

def equation({params_str}):
{function_body}

def evaluate_function(inputs, outputs, var_names):
    \"\"\"V2ç‰ˆæœ¬ï¼šè¯„ä¼°å‡½æ•°æ€§èƒ½ - ä½¿ç”¨BFGSä¼˜åŒ–å‚æ•°\"\"\"
    try:
        def loss_function(params):
            try:
                # ğŸ”¥ æŒ‰ç…§æ— RLç‰ˆæœ¬çš„æ–¹å¼ï¼Œç›´æ¥ä¼ é€’æ•´ä¸ªæ•°ç»„
                if len(var_names) == 2:  # x, v (oscillator)
                    x_data = inputs[:, 0]
                    v_data = inputs[:, 1] 
                    predictions = equation(x_data, v_data, params)
                elif len(var_names) == 4:  # b, s, temp, pH (bactgrow)
                    b_data = inputs[:, 0]
                    s_data = inputs[:, 1]
                    temp_data = inputs[:, 2]
                    pH_data = inputs[:, 3]
                    predictions = equation(b_data, s_data, temp_data, pH_data, params)
                elif len(var_names) == 2 and 'strain' in var_names:  # strain, temp (stressstrain)
                    strain_data = inputs[:, 0]
                    temp_data = inputs[:, 1]
                    predictions = equation(strain_data, temp_data, params)
                else:
                    # é€šç”¨å¤„ç†ï¼šä¼ é€’æ‰€æœ‰åˆ—ä½œä¸ºå‚æ•°
                    args = [inputs[:, j] for j in range(inputs.shape[1])]
                    predictions = equation(*args, params)
                
                # ç¡®ä¿predictionsæ˜¯numpyæ•°ç»„
                predictions = np.asarray(predictions, dtype=np.float64)
                
                # å¤„ç†æ ‡é‡è¿”å›å€¼
                if predictions.ndim == 0:
                    predictions = np.full_like(outputs, float(predictions))
                
                # è®¡ç®—MSE
                mse = np.mean((predictions - outputs) ** 2)
                return float(mse) if np.isfinite(mse) else 1e6
                
            except Exception as e:
                return 1e6
        
        # ğŸ”¥ BFGSå‚æ•°ä¼˜åŒ–ï¼ˆæ¨¡ä»¿æ— RLç‰ˆæœ¬ï¼‰
        initial_params = np.ones(10)
        result = minimize(loss_function, initial_params, method='BFGS')
        
        # è·å–ä¼˜åŒ–åçš„å‚æ•°å’ŒæŸå¤±
        optimized_params = result.x
        optimized_loss = result.fun
        
        # å¤„ç†ä¼˜åŒ–å¤±è´¥çš„æƒ…å†µ
        if np.isnan(optimized_loss) or np.isinf(optimized_loss) or not result.success:
            print(f"âš ï¸ V2 BFGSä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            optimized_params = initial_params
            optimized_loss = loss_function(initial_params)
        
        return float(optimized_loss), optimized_params
        
    except Exception as e:
        print(f"âŒ V2å‡½æ•°æ‰§è¡Œé”™è¯¯: {{e}}")
        return 1e6, np.ones(10)
"""
    
    return program


def execute_and_compute_mse_v2(program: str, inputs: np.ndarray, outputs: np.ndarray, var_names: list) -> tuple[float, np.ndarray]:
    """V2ç‰ˆæœ¬ï¼šåœ¨å®‰å…¨ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºå¹¶è®¡ç®—MSE"""
    
    try:
        # æ‰§è¡Œç¨‹åº
        all_globals_namespace = {
            'np': np,
            'numpy': np,
            'math': math
        }
        
        # æ‰§è¡Œç¨‹åº
        exec(program, all_globals_namespace)
        
        # è·å–è¯„ä¼°å‡½æ•°
        evaluate_function = all_globals_namespace['evaluate_function']
        
        # è°ƒç”¨è¯„ä¼°å‡½æ•°
        mse, params_used = evaluate_function(inputs, outputs, var_names)
        
        return mse, params_used
        
    except Exception as e:
        print(f"âŒ V2ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1e6, None


def _load_training_data_from_path(data_path: str | None) -> Tuple[np.ndarray | None, np.ndarray | None, List[str] | None]:
    if not data_path:
        return None, None, None
    try:
        import pandas as pd
        df = pd.read_csv(data_path)
        data = df.values
        # å‡è®¾æœ€åä¸€åˆ—ä¸ºè¾“å‡ºï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬
        X = data[:, :-1]
        y = data[:, -1].reshape(-1)
        var_names = df.columns[:-1].tolist()
        return X, y, var_names
    except Exception:
        return None, None, None


def _estimate_complexity_from_body(function_body: str) -> float:
    """
    ğŸ”¥ å‡çº§ç‰ˆå¤æ‚åº¦ä¼°è®¡ï¼šAST + å­æ ‘å¤ç”¨ + å¸¸æ•°MDL + åµŒå¥—æ·±åº¦ + åˆ†æ®µ/ä¸å¯å¾®ç»“æ„
    åŸºäºASTåˆ†æè€Œéç®€å•æ­£åˆ™ï¼Œæ›´å‡†ç¡®åæ˜ è¡¨è¾¾å¼çš„ç»“æ„å¤æ‚åº¦
    """
    return estimate_complexity_from_body_v3(function_body)


# â€”â€” åŸºç¡€æƒé‡ï¼ˆå‚ç…§ PySR: è¶…è¶Šå‡½æ•°æ›´è´µï¼‰ â€”â€”
OP_WEIGHTS = {
    'Add': 1.0, 'Sub': 1.0,
    'Mult': 1.5, 'Div': 2.0, 'FloorDiv': 2.0, 'Mod': 2.5,
    'Pow': 3.0,
}
# å¸¸è§æ•°å­¦å‡½æ•°ä»£ä»·ï¼›å¯ç»§ç»­æ‰©å……
FUNC_WEIGHTS = {
    "sin": 2.0, "cos": 2.0, "tan": 3.0,
    "exp": 4.0, "log": 4.0, "sqrt": 3.0, "abs": 2.0, "tanh": 3.0,
    "sinh": 3.0, "cosh": 3.0, "atan": 3.0, "asin": 3.0, "acos": 3.0,
}


def estimate_complexity_from_body_v3(function_body: str) -> float:
    """
    æ›´ç²¾ç»†çš„å¤æ‚åº¦ä¼°è®¡ï¼ˆAST + å­æ ‘å¤ç”¨ + å¸¸æ•°MDL + åµŒå¥—æ·±åº¦ + åˆ†æ®µ/ä¸å¯å¾®ç»“æ„ï¼‰
    è¿”å›æ ‡é‡å¤æ‚åº¦ Cï¼ˆè¶Šå¤§è¶Šå¤æ‚ï¼‰
    """
    if not function_body or not isinstance(function_body, str):
        return 0.0

    # æ„é€ å¯è§£æçš„å‡å‡½æ•°ï¼Œä¿è¯ç¼©è¿›æ­£ç¡®
    code = f"def __eq__(x, y, z, params):\n{function_body}"
    try:
        tree = ast.parse(code)
    except SyntaxError:
        # è¯­æ³•ä¸é€šæ—¶ç»™é«˜å¤æ‚åº¦
        return 100.0

    # æ‰¾åˆ°ç›®æ ‡å‡½æ•°ä½“
    fnode = None
    for n in tree.body:
        if isinstance(n, ast.FunctionDef) and n.name == "__eq__":
            fnode = n
            break
    
    if fnode is None:
        return 100.0

    # â€”â€” çŠ¶æ€å®¹å™¨ â€”â€”
    stats = {
        "op_cost": 0.0,              # åŠ æƒç®—å­æˆæœ¬
        "func_cost": 0.0,            # åŠ æƒå‡½æ•°æˆæœ¬
        "depth_max": 0,              # æœ€å¤§åµŒå¥—æ·±åº¦
        "piecewise_cnt": 0,          # åˆ†æ®µ/ä¸å¯å¾®ç»“æ„å‡ºç°æ¬¡æ•°
        "pow_max_k": 1,              # æœ€å¤§å¹‚é˜¶
        "const_bits": 0.0,           # å¸¸æ•°æè¿°é•¿åº¦ï¼ˆMDL è¿‘ä¼¼ï¼‰
        "unique_subtrees": 0,        # DAG å”¯ä¸€å­æ ‘è®¡æ•°
        "total_subtrees": 0,         # å­æ ‘æ€»æ•°ï¼ˆç”¨äºå¤ç”¨ç‡ä¼°è®¡ï¼‰
        "poly_terms": 0,             # ä¼°ç®—å¤šé¡¹å¼é¡¹æ•°
    }

    # â€”â€” å­æ ‘å“ˆå¸Œï¼šè¡¡é‡ DAG å‹ç¼©æ€§/å”¯ä¸€å­å¼æ•°é‡ â€”â€”
    from collections import defaultdict
    counter = defaultdict(int)
    
    def hash_subtree(n):
        # åŸºäºèŠ‚ç‚¹ç±»å‹ä¸å­ç»“æ„çš„é€’å½’å“ˆå¸Œï¼ˆæ–‡æœ¬åŒ–ï¼‰ï¼›åªåšå¯å‘å¼
        if isinstance(n, ast.Constant) and isinstance(n.value, (int, float)):
            return f"Const({repr(n.value)})"
        elif hasattr(ast, 'Num') and isinstance(n, ast.Num):  # Python < 3.8 å…¼å®¹
            return f"Const({repr(n.n)})"
        
        label = type(n).__name__
        fields = []
        for name, val in ast.iter_fields(n):
            if isinstance(val, ast.AST):
                fields.append((name, hash_subtree(val)))
            elif isinstance(val, list):
                fields.append((name, tuple(hash_subtree(x) for x in val if isinstance(x, ast.AST))))
            elif isinstance(val, (str, int, float, bool, type(None))):
                # ä¸æŠŠè¡Œå·/åˆ—å·ç­‰å…ƒä¿¡æ¯çº³å…¥
                if name not in ("lineno", "col_offset", "end_lineno", "end_col_offset", "id", "arg"):
                    fields.append((name, val))
        key = f"{label}:{tuple(fields)}"
        counter[key] += 1
        return key

    # â€”â€” éå†ï¼šç»Ÿè®¡å„åˆ†é‡ + è®°å½•æ·±åº¦/å¹‚é˜¶/åˆ†æ®µç»“æ„ â€”â€”
    def walk(n, depth=0):
        stats["depth_max"] = max(stats["depth_max"], depth)
        stats["total_subtrees"] += 1
        key = hash_subtree(n)  # è§¦å‘è®¡æ•°

        # äºŒå…ƒç®—å­
        if isinstance(n, ast.BinOp):
            op_name = type(n.op).__name__
            stats["op_cost"] += OP_WEIGHTS.get(op_name, 1.5)
            if isinstance(n.op, ast.Pow):
                # è§£æå¹‚é˜¶ï¼ˆä»…å½“æŒ‡æ•°æ˜¯å¸¸æ•°æ—¶å¯é ï¼‰
                k = _extract_integer_pow(n)
                if k is not None:
                    stats["pow_max_k"] = max(stats["pow_max_k"], k)

        # å‡½æ•°è°ƒç”¨
        if isinstance(n, ast.Call):
            fname = _get_call_name(n)
            if fname:
                stats["func_cost"] += FUNC_WEIGHTS.get(fname, 3.0)

        # åˆ†æ®µ/ä¸å¯å¾®ï¼šif-else, æ¯”è¾ƒ, æ¡ä»¶è¡¨è¾¾å¼, abs()
        if isinstance(n, (ast.If, ast.IfExp, ast.Compare)):
            stats["piecewise_cnt"] += 1
        if isinstance(n, ast.Call):
            fname = _get_call_name(n)
            if fname in ("abs",):
                stats["piecewise_cnt"] += 1

        # å¸¸æ•°çš„ MDL è¿‘ä¼¼
        if isinstance(n, ast.Constant) and isinstance(n.value, (int, float)):
            stats["const_bits"] += _constant_description_bits(n.value)
        elif hasattr(ast, 'Num') and isinstance(n, ast.Num):  # Python < 3.8 å…¼å®¹
            stats["const_bits"] += _constant_description_bits(n.n)

        # é€’å½’å­èŠ‚ç‚¹
        for child in ast.iter_child_nodes(n):
            walk(child, depth + 1)

    for stmt in fnode.body:
        walk(stmt, depth=1)

    # ç»Ÿè®¡å”¯ä¸€å­æ ‘æ•°ï¼ˆDAGï¼‰
    stats["unique_subtrees"] = sum(1 for k, c in counter.items() if c >= 1)

    # ä¼°ç®—å¤šé¡¹å¼é¡¹æ•°ï¼ˆå¯å‘ï¼šæŒ‰ "åŠ æ³•é“¾ + å¹‚è¡¨è¾¾å¼" ç²—ç•¥ä¼°è®¡ï¼‰
    stats["poly_terms"] = _estimate_poly_terms(counter)

    # â€”â€” ç»„åˆå¤æ‚åº¦ï¼ˆæƒé‡å¯è°ƒï¼‰ â€”â€”
    # æ·±åº¦é¢å¤–æƒ©ç½šï¼šæ·±å±‚æ“ä½œåœ¨è®¡ç®—å’Œå¯è§£é‡Šæ€§ä¸Šéƒ½æ›´éš¾
    depth_cost = 0.5 * stats["depth_max"]
    # DAGï¼šå”¯ä¸€å­æ ‘è¶Šå¤šè¶Šå¤æ‚ï¼›å¯ç”¨ "å”¯ä¸€/æ€»æ•°" çš„æ¯”å€¼æ¥åº¦é‡å¯å‹ç¼©æ€§
    if stats["total_subtrees"] > 0:
        dag_ratio = stats["unique_subtrees"] / float(stats["total_subtrees"])
    else:
        dag_ratio = 1.0
    dag_cost = 5.0 * dag_ratio

    # å¹‚é˜¶ã€åˆ†æ®µã€é¡¹æ•°
    pow_cost = 0.3 * max(0, stats["pow_max_k"] - 1)
    piece_cost = 1.5 * stats["piecewise_cnt"]
    terms_cost = 0.2 * stats["poly_terms"]

    # åˆæˆæ€»å¤æ‚åº¦
    C = (
        stats["op_cost"]
        + stats["func_cost"]
        + depth_cost
        + dag_cost
        + 0.05 * stats["const_bits"]  # å¸¸æ•°çš„æè¿°é•¿åº¦ï¼ˆä½æ•°/ç²¾åº¦è¶Šé«˜è¶Šè´µï¼‰
        + pow_cost
        + piece_cost
        + terms_cost
    )
    return float(C)


# â€”â€” è¾…åŠ©ï¼šæå–å‡½æ•°å â€”â€”
def _get_call_name(node):
    """æå–å‡½æ•°è°ƒç”¨çš„åç§°"""
    if isinstance(node.func, ast.Name):
        return node.func.id
    if isinstance(node.func, ast.Attribute):
        return node.func.attr
    return None


# â€”â€” è¾…åŠ©ï¼šæå–å¹‚æŒ‡æ•°ï¼ˆè‹¥ä¸ºæ•´æ•°å¸¸æ•°ï¼‰ â€”â€”
def _extract_integer_pow(binop):
    """æå–å¹‚è¿ç®—çš„æŒ‡æ•°ï¼ˆå¦‚æœæ˜¯æ•´æ•°å¸¸æ•°ï¼‰"""
    if isinstance(binop.op, ast.Pow):
        if isinstance(binop.right, ast.Constant) and isinstance(binop.right.value, (int, float)):
            try:
                k = int(binop.right.value)
                return k if k >= 1 else None
            except Exception:
                return None
        elif hasattr(ast, 'Num') and isinstance(binop.right, ast.Num):  # Python < 3.8 å…¼å®¹
            try:
                k = int(binop.right.n)
                return k if k >= 1 else None
            except Exception:
                return None
    return None


# â€”â€” è¾…åŠ©ï¼šMDL è¿‘ä¼¼ï¼ˆå¸¸æ•°çš„æè¿°é•¿åº¦ï¼Œä½æ•°+æ•°é‡çº§ï¼‰ â€”â€”
def _constant_description_bits(v) -> float:
    """è®¡ç®—å¸¸æ•°çš„æè¿°é•¿åº¦ï¼ˆMDLè¿‘ä¼¼ï¼‰"""
    v = float(v)
    if v == 0.0:
        return 1.0
    # ä½æ•°æƒ©ç½šï¼šå°æ•°çš„æœ‰æ•ˆæ•°å­—è¶Šå¤šè¶Šè´µï¼ˆä»¥åè¿›åˆ¶è¿‘ä¼¼ï¼‰
    s = f"{v:.12g}"  # é™12ä½æœ‰æ•ˆæ•°å­—ï¼Œé¿å…ç§‘å­¦è®¡æ•°æ³•æç«¯
    digits = len(re.sub(r"[^0-9]", "", s))
    # æ•°é‡çº§æƒ©ç½šï¼š|log10(|v|)| è¶Šå¤§è¶Šè´µï¼ˆé˜²è¶…å¤§/è¶…å°å¸¸æ•°ï¼‰
    magnitude = abs(math.log10(abs(v))) if v != 0 else 0.0
    return digits + 2.0 * magnitude


# â€”â€” è¾…åŠ©ï¼šä¼°ç®—å¤šé¡¹å¼é¡¹æ•°ï¼ˆå¯å‘å¼ï¼šåŸºäºå­æ ‘é”®ï¼‰ â€”â€”
def _estimate_poly_terms(subtree_counter) -> int:
    """ä¼°ç®—å¤šé¡¹å¼é¡¹æ•°ï¼ˆå¯å‘å¼æ–¹æ³•ï¼‰"""
    # ç»Ÿè®¡å‡ºç° "Add:" çš„å­æ ‘ä¸ªæ•°ä½œä¸ºé¡¹åˆ†è£‚çš„ç²—ç•¥åº¦é‡
    terms = sum(1 for k in subtree_counter if k.startswith("BinOp:") and "Add" in k)
    return max(0, terms)


def _physical_consistency_v2(function_body: str, var_names: List[str], X: np.ndarray, y: np.ndarray) -> float:
    """V2ç‰ˆæœ¬ï¼šç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥"""
    try:
        # ç®€åŒ–çš„ç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥
        if 'log(' in function_body:
            # log éœ€è¦æ­£è¾“å…¥ï¼Œæ£€æŸ¥æ˜¯å¦å¯èƒ½å‡ºç°éæ­£æ•°
            if np.any(X <= 0):
                return 0.6
        return 1.0
    except Exception:
        return 0.2


def _estimate_token_length(
    text: str,
    *,
    # å¯é€‰ï¼šä¼ å…¥çœŸå® tokenizerï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
    hf_tokenizer=None,     # ä¾‹å¦‚ transformers çš„ AutoTokenizer() å®ä¾‹
    tiktoken_encoder=None, # ä¾‹å¦‚ tiktoken.encoding_for_model(...).encode
    tokenizer_encode_fn=None,  # ä»»ä½•å¯è°ƒç”¨çš„ encode å‡½æ•°
    model_family: str = "qwen",  # 'qwen' | 'openai' | 'llama' | 'generic'
) -> int:
    """
    ğŸ”¥ å‡çº§ç‰ˆtokené•¿åº¦ä¼°è®¡ï¼šç²¾ç¡®ä¼˜å…ˆ + å¯å‘å¼å…œåº•
    1) è‹¥æä¾›çœŸå® tokenizerï¼Œç›´æ¥è¿”å›ç²¾ç¡®é•¿åº¦
    2) å¦åˆ™é‡‡ç”¨"ç±»åˆ«åˆ†æ®µ + å­—èŠ‚/4"çš„æ··åˆå¯å‘å¼ï¼ŒæŒ‰æ¨¡å‹å®¶æ—è°ƒç³»æ•°
    """
    
    # ---- 1) ç²¾ç¡®è®¡æ•°ï¼ˆä¼˜å…ˆï¼‰ ----
    if text is None or text == "":
        return 0
    
    # ğŸ”¥ å°è¯•è‡ªåŠ¨åŠ è½½Qwen3-8Bçš„tokenizer
    if hf_tokenizer is None and tiktoken_encoder is None and tokenizer_encode_fn is None:
        hf_tokenizer = _get_qwen_tokenizer()
    
    try:
        # transformers
        if hf_tokenizer is not None and hasattr(hf_tokenizer, "encode"):
            return int(len(hf_tokenizer.encode(text)))
        # tiktoken
        if tiktoken_encoder is not None and hasattr(tiktoken_encoder, "encode"):
            return int(len(tiktoken_encoder.encode(text)))
        # ä»»æ„å¯è°ƒç”¨ encode
        if callable(tokenizer_encode_fn):
            return int(len(tokenizer_encode_fn(text)))
    except Exception as e:
        # è‹¥å¤±è´¥ï¼Œé€€å›å¯å‘å¼
        print(f"âš ï¸ Tokenizerå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ä¼°è®¡: {e}")
        pass

    # ---- 2) æ”¹è¿›å¯å‘å¼ï¼ˆç±»åˆ«åˆ†æ®µ + UTF-8å­—èŠ‚ï¼‰----
    return _estimate_token_length_heuristic(text, model_family)


def _get_qwen_tokenizer():
    """å°è¯•åŠ è½½Qwen3-8Bçš„tokenizer"""
    try:
        from transformers import AutoTokenizer
        # å°è¯•ä»æœ¬åœ°Qwen3-8Bç›®å½•åŠ è½½
        qwen_path = "/storage/home/westlakeLab/zhangjunlei/Qwen3-8B"
        if os.path.exists(qwen_path):
            tokenizer = AutoTokenizer.from_pretrained(qwen_path, trust_remote_code=True)
            print(f"âœ… æˆåŠŸåŠ è½½Qwen3-8B tokenizer: {qwen_path}")
            return tokenizer
    except Exception as e:
        print(f"âš ï¸ æ— æ³•ä» {qwen_path} åŠ è½½Qwen3-8B tokenizer: {e}")
    return None


def _estimate_token_length_heuristic(text: str, model_family: str = "qwen") -> int:
    """
    å¯å‘å¼tokené•¿åº¦ä¼°è®¡ï¼šç±»åˆ«åˆ†æ®µ + UTF-8å­—èŠ‚æ··åˆæ¨¡å‹
    """
    if not text:
        return 0
    
    # 2.1 ç±»åˆ«åˆ’åˆ†ï¼ˆå°½é‡äº’æ–¥ï¼‰
    # CJK ç»Ÿä¸€è¡¨æ„ & æ‰©å±•ã€å‡åã€éŸ©æ–‡éŸ³èŠ‚
    re_cjk = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF\u3040-\u30FF\uAC00-\uD7A3]")
    # è¿‘ä¼¼ Emoji / è¡¨æƒ…ç¬¦ï¼ˆè¦†ç›–å¸¸è§åŒºæ®µï¼‰
    re_emoji = re.compile(r"[\U0001F000-\U0001FAFF\U00002702-\U000027B0]")
    # URL / Emailï¼ˆURL å…ˆè¡ŒåŒ¹é…ï¼Œé¿å…è¢«æŒ‰è¯æ‹†æ•£ï¼‰
    re_url  = re.compile(r"https?://[^\s]+|www\.[^\s]+")
    re_mail = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
    # æ•°å­—ï¼ˆå«å°æ•°/ç§‘å­¦è®¡æ•°ï¼‰
    re_num  = re.compile(r"[+-]?(?:\d+\.\d+|\d+\.|\.\d+|\d+)(?:[eE][+-]?\d+)?")
    # ä»£ç /æ“ä½œç¬¦ä¸²
    re_ops  = re.compile(r"[+\-*/=<>!%&|^~]+")
    # è›‡å½¢/é©¼å³°è¯ï¼ˆä¼˜å…ˆæŠ“é•¿è¯ï¼Œé¿å…è¿‡åº¦åˆ‡ç¢ï¼‰
    re_word = re.compile(r"[A-Za-z]+(?:[_\-][A-Za-z0-9]+)*|[A-Za-z][a-z0-9]+(?:[A-Z][a-z0-9]+)+")
    # å…¶å®ƒå¯è§ ASCII æ ‡ç‚¹
    re_punc = re.compile(r"[.,;:!?â€¦â€”â€“()\[\]{}\<>\"'`""''#@$]")
    # ç©ºç™½
    re_space = re.compile(r"\s+")

    text_remaining = text

    def _pop_all(pattern):
        nonlocal text_remaining
        items = pattern.findall(text_remaining)
        text_remaining = pattern.sub(" ", text_remaining)  # ç”¨ç©ºæ ¼å ä½ï¼Œé¿å…è¿é”å½±å“
        return items

    urls  = _pop_all(re_url)
    mails = _pop_all(re_mail)
    nums  = _pop_all(re_num)
    opss  = _pop_all(re_ops)
    words = _pop_all(re_word)
    # å…ˆå¼¹å‡º emojiï¼Œå†åŒ¹é… CJKï¼ˆé¿å…é‡å¤è®¡æ•°ï¼‰
    emojis = _pop_all(re_emoji)
    cjks   = _pop_all(re_cjk)
    puncs  = _pop_all(re_punc)
    spaces = _pop_all(re_space)

    # å‰©ä½™é›¶æ•£å­—ç¬¦ï¼ˆæ··åˆï¼šå¯èƒ½æ˜¯ç¨€æœ‰ç¬¦å·ã€æ§åˆ¶ç¬¦ç­‰ï¼‰
    leftovers = [c for c in text_remaining if not c.isspace()]

    # 2.2 æ¨¡å‹å®¶æ—ç³»æ•°ï¼ˆå¯æŒ‰ç»éªŒ/æ ‡å®šå¾®è°ƒï¼‰
    if model_family.lower() in ("qwen", "qwen2", "qwen3"):
        coef = dict(
            en_char_per_tok = 4.0,   # è‹±æ–‡ï¼š4å­—ç¬¦/Token
            digit_char_per_tok = 3.0,# æ•°å­—ï¼š3å­—ç¬¦/Token
            cjk_tok_per_char = 0.65, # ä¸­æ–‡æ—¥æ–‡éŸ©æ–‡ï¼š~0.6â€“0.8 Token/å­—ï¼ˆQwen BPE å¸¸è§åŒºé—´ï¼‰
            url_char_per_tok = 3.0,  # URL æ›´ç¢ï¼š3å­—ç¬¦/Token
            mail_char_per_tok= 3.2,  # Email
            ops_char_per_tok = 2.0,  # æ“ä½œç¬¦ï¼š2å­—ç¬¦/Token
            punc_char_per_tok= 2.5,  # æ ‡ç‚¹ï¼š2.5å­—ç¬¦/Token
            space_char_per_tok=10.0, # ç©ºç™½ï¼š10å­—ç¬¦/Tokenï¼ˆå¤§å¤šå¹¶å…¥ç›¸é‚» token çš„å‰å¯¼ç©ºæ ¼ï¼‰
            emoji_tok_per_char=1.3,  # emojiï¼š1.3 Token/å­—ç¬¦
            leftover_char_per_tok=3.2,
            mix_byte_weight = 0.30,  # ä¸"å­—èŠ‚/4"èåˆçš„æƒé‡
        )
    elif model_family.lower() in ("openai", "gpt", "o"):
        coef = dict(
            en_char_per_tok = 4.0,
            digit_char_per_tok = 2.8,
            cjk_tok_per_char = 1.0,  # tiktoken ä¸Šä¸­æ–‡æ›´æ¥è¿‘ 1 Token/å­—
            url_char_per_tok = 2.6,
            mail_char_per_tok= 2.8,
            ops_char_per_tok = 1.8,
            punc_char_per_tok= 2.2,
            space_char_per_tok=12.0,
            emoji_tok_per_char=1.6,
            leftover_char_per_tok=3.0,
            mix_byte_weight = 0.35,
        )
    else:  # 'llama'/'generic' å…œåº•
        coef = dict(
            en_char_per_tok = 4.0,
            digit_char_per_tok = 3.0,
            cjk_tok_per_char = 0.9,
            url_char_per_tok = 2.8,
            mail_char_per_tok= 3.0,
            ops_char_per_tok = 2.0,
            punc_char_per_tok= 2.5,
            space_char_per_tok=10.0,
            emoji_tok_per_char=1.5,
            leftover_char_per_tok=3.2,
            mix_byte_weight = 0.30,
        )

    # 2.3 å­ç±»ä¼°è®¡ï¼ˆæŠŠ"å­—ç¬¦/æ¯Token"æˆ–"Token/å­—ç¬¦"ç»Ÿä¸€æ¢ç®—æˆ Token è®¡æ•°ï¼‰
    # è‹±æ–‡è¯æŒ‰å­—ç¬¦æ•°ä¼°è®¡ï¼ˆæ‹¼åˆè›‡å½¢/é©¼å³°åæ›´æ¥è¿‘çœŸå® BPEï¼‰
    en_chars = sum(len(w) for w in words)
    tokens_en   = en_chars / coef["en_char_per_tok"]
    tokens_num  = sum(len(s) for s in nums)  / coef["digit_char_per_tok"]
    tokens_url  = sum(len(s) for s in urls)  / coef["url_char_per_tok"]
    tokens_mail = sum(len(s) for s in mails) / coef["mail_char_per_tok"]
    tokens_ops  = sum(len(s) for s in opss)  / coef["ops_char_per_tok"]
    tokens_punc = sum(len(s) for s in puncs) / coef["punc_char_per_tok"]
    tokens_space= sum(len(s) for s in spaces)/ coef["space_char_per_tok"]
    tokens_cjk  = sum(len(s) for s in cjks)  * coef["cjk_tok_per_char"]
    tokens_emoji= sum(len(s) for s in emojis)* coef["emoji_tok_per_char"]
    tokens_left = len(leftovers) / coef["leftover_char_per_tok"]

    est_class = (
        tokens_en + tokens_num + tokens_url + tokens_mail +
        tokens_ops + tokens_punc + tokens_space + tokens_cjk +
        tokens_emoji + tokens_left
    )

    # 2.4 å­—èŠ‚/4 èåˆï¼ˆtiktoken æ–‡æ¡£ç»éªŒï¼šå¹³å‡æ¯ Token ~4å­—èŠ‚ï¼‰
    est_bytes = len(text.encode("utf-8")) / 4.0
    mix_w = float(coef["mix_byte_weight"])
    est = (1.0 - mix_w) * est_class + mix_w * est_bytes

    # 2.5 ä¿æŠ¤æ€§çº¦æŸï¼ˆé¿å…æç«¯ä½ä¼°/é«˜ä¼°ï¼‰
    # - Token ä¸å¯èƒ½è¶…è¿‡"å¯è§å­—ç¬¦æ•° * 2"ï¼ˆæç«¯ç¢è£‚ä¸Šé™ï¼Œå®½æ¾ï¼‰
    # - ä¹Ÿä¸åº”å°äº"éç©ºå­—ç¬¦æ•° / 8"ï¼ˆæç«¯åˆå¹¶ä¸‹é™ï¼Œå®½æ¾ï¼‰
    nonspace = len([c for c in text if not c.isspace()])
    upper = 2.0 * nonspace + 16  # åŠ å¸¸æ•°é¡¹åº”å¯¹å¾ˆçŸ­æ–‡æœ¬
    lower = max(1.0, nonspace / 8.0)
    est = max(lower, min(est, upper))

    return int(math.ceil(est))