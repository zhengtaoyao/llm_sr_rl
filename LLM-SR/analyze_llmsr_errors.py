#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æåŸç‰ˆLLM-SRè¿›åŒ–æœç´¢çš„é”™è¯¯ç»Ÿè®¡è„šæœ¬
ç”¨äºæŸ¥çœ‹legal/illegal functionæ•°é‡å’Œé”™è¯¯ç±»å‹
"""

import os
import json
import re
import numpy as np
import argparse
from typing import Dict, List, Tuple
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator

def analyze_log_file(log_file: str) -> Dict:
    """åˆ†æä¸»æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯"""
    print(f'ğŸ” åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file}')
    
    with open(log_file, 'r') as f:
        content = f.read()
    
    # ç»Ÿè®¡ä¸åŒç±»å‹çš„é”™è¯¯
    execution_errors = re.findall(r'Execution Error: (.+)', content)
    score_none_count = content.count('Score        : None')
    successful_scores = re.findall(r'Score        : (-?\d+\.?\d*)', content)
    
    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    error_types = {}
    for error in execution_errors:
        if error in error_types:
            error_types[error] += 1
        else:
            error_types[error] = 1
    
    # å¤„ç†æˆåŠŸåˆ†æ•°
    scores = []
    if successful_scores:
        scores = [float(s) for s in successful_scores if s != 'None']
    
    total_samples = len(execution_errors) + len(scores)
    success_rate = len(scores) / total_samples if total_samples > 0 else 0
    
    return {
        'total_samples': total_samples,
        'execution_errors': len(execution_errors),
        'score_none_count': score_none_count,
        'successful_evaluations': len(scores),
        'success_rate': success_rate,
        'error_types': error_types,
        'scores': scores
    }

def analyze_samples_json(samples_dir: str) -> Dict:
    """åˆ†æsamplesç›®å½•ä¸­çš„JSONæ–‡ä»¶"""
    print(f'ğŸ“Š åˆ†ææ ·æœ¬ç›®å½•: {samples_dir}')
    
    total_samples = 0
    success_samples = 0
    failed_samples = 0
    scores = []
    
    if not os.path.exists(samples_dir):
        print(f'âš ï¸ æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {samples_dir}')
        return {}
    
    for filename in os.listdir(samples_dir):
        if filename.endswith('.json'):
            total_samples += 1
            try:
                with open(os.path.join(samples_dir, filename), 'r') as f:
                    data = json.load(f)
                    if data.get('score') is not None:
                        success_samples += 1
                        scores.append(float(data['score']))
                    else:
                        failed_samples += 1
            except Exception as e:
                print(f'âŒ è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}')
                failed_samples += 1
    
    return {
        'total_samples': total_samples,
        'success_samples': success_samples,
        'failed_samples': failed_samples,
        'success_rate': success_samples / total_samples if total_samples > 0 else 0,
        'scores': scores
    }

def analyze_tensorboard(log_dir: str) -> Dict:
    """åˆ†æTensorBoardæ•°æ®"""
    print(f'ğŸ“ˆ åˆ†æTensorBoardæ•°æ®: {log_dir}')
    
    try:
        ea = EventAccumulator(log_dir)
        ea.Reload()
        
        result = {
            'available_tags': ea.Tags()['scalars'],
            'legal_illegal_data': {},
            'time_data': {},
            'best_score_data': {}
        }
        
        # è·å–Legal/Illegal Functionç»Ÿè®¡
        if 'Legal_Illegal Function/legal function num' in ea.Tags()['scalars']:
            legal_data = ea.Scalars('Legal_Illegal Function/legal function num')
            illegal_data = ea.Scalars('Legal_Illegal Function/illegal function num')
            
            result['legal_illegal_data'] = {
                'legal_functions': legal_data[-1].value if legal_data else 0,
                'illegal_functions': illegal_data[-1].value if illegal_data else 0,
                'legal_history': [(s.step, s.value) for s in legal_data],
                'illegal_history': [(s.step, s.value) for s in illegal_data]
            }
        
        # è·å–æ—¶é—´ç»Ÿè®¡
        if 'Total Sample_Evaluate Time/sample time' in ea.Tags()['scalars']:
            sample_time_data = ea.Scalars('Total Sample_Evaluate Time/sample time')
            eval_time_data = ea.Scalars('Total Sample_Evaluate Time/evaluate time')
            
            result['time_data'] = {
                'total_sample_time': sample_time_data[-1].value if sample_time_data else 0,
                'total_evaluate_time': eval_time_data[-1].value if eval_time_data else 0
            }
        
        # è·å–æœ€ä½³åˆ†æ•°
        if 'Best Score of Function' in ea.Tags()['scalars']:
            best_score_data = ea.Scalars('Best Score of Function')
            result['best_score_data'] = {
                'current_best': best_score_data[-1].value if best_score_data else None,
                'history': [(s.step, s.value) for s in best_score_data]
            }
        
        return result
    
    except Exception as e:
        print(f'âŒ TensorBoardåˆ†æå¤±è´¥: {e}')
        return {}

def print_error_summary(experiment_path: str):
    """æ‰“å°é”™è¯¯æ‘˜è¦æŠ¥å‘Š"""
    print('\n' + '='*60)
    print('ğŸ§¬ LLM-SR åŸç‰ˆè¿›åŒ–æœç´¢ - é”™è¯¯åˆ†ææŠ¥å‘Š')
    print('='*60)
    
    # åˆ†æä¸»æ—¥å¿—
    log_files = [f for f in os.listdir('llmsr_logs') if experiment_path in f and f.endswith('.log')]
    if log_files:
        latest_log = sorted(log_files)[-1]
        log_path = os.path.join('llmsr_logs', latest_log)
        log_results = analyze_log_file(log_path)
        
        print(f'\nğŸ“‹ ä¸»æ—¥å¿—åˆ†æ ({latest_log}):')
        print(f'  æ€»æ ·æœ¬æ•°: {log_results["total_samples"]}')
        print(f'  æˆåŠŸè¯„ä¼°: {log_results["successful_evaluations"]} ({log_results["success_rate"]:.1%})')
        print(f'  æ‰§è¡Œå¤±è´¥: {log_results["execution_errors"]} ({(1-log_results["success_rate"]):.1%})')
        
        print(f'\nğŸ” é”™è¯¯ç±»å‹è¯¦æƒ…:')
        for error_type, count in log_results['error_types'].items():
            print(f'  - {error_type}: {count}æ¬¡')
    
    # åˆ†ææ ·æœ¬JSON
    samples_path = f'logs/{experiment_path}/samples'
    if os.path.exists(samples_path):
        json_results = analyze_samples_json(samples_path)
        
        print(f'\nğŸ“Š æ ·æœ¬JSONåˆ†æ:')
        print(f'  æ€»æ ·æœ¬æ•°: {json_results["total_samples"]}')
        print(f'  æˆåŠŸæ ·æœ¬: {json_results["success_samples"]} ({json_results["success_rate"]:.1%})')
        print(f'  å¤±è´¥æ ·æœ¬: {json_results["failed_samples"]} ({(1-json_results["success_rate"]):.1%})')
    
    # åˆ†æTensorBoardæ•°æ®
    tb_path = f'logs/{experiment_path}'
    if os.path.exists(tb_path):
        tb_results = analyze_tensorboard(tb_path)
        
        if 'legal_illegal_data' in tb_results and tb_results['legal_illegal_data']:
            legal_data = tb_results['legal_illegal_data']
            print(f'\nğŸ“ˆ TensorBoardç»Ÿè®¡:')
            print(f'  Legal functions: {legal_data["legal_functions"]}')
            print(f'  Illegal functions: {legal_data["illegal_functions"]}')
            
            total = legal_data["legal_functions"] + legal_data["illegal_functions"]
            if total > 0:
                legal_rate = legal_data["legal_functions"] / total
                print(f'  Legal rate: {legal_rate:.1%}')
        
        if 'best_score_data' in tb_results and tb_results['best_score_data']:
            best_data = tb_results['best_score_data']
            if best_data['current_best'] is not None:
                print(f'\nğŸ† æœ€ä½³åˆ†æ•°: {best_data["current_best"]:.8f}')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='åˆ†æLLM-SRé”™è¯¯ç»Ÿè®¡')
    parser.add_argument('--experiment', type=str, default='oscillator1_qwen3_8b_evolution',
                        help='å®éªŒåç§° (ä¾‹å¦‚: oscillator1_qwen3_8b_evolution)')
    
    args = parser.parse_args()
    print_error_summary(args.experiment)
    
    print(f'\nğŸ’¡ æŸ¥çœ‹æ–¹æ³•:')
    print(f'  1. æŸ¥çœ‹ä¸»æ—¥å¿—: tail -f llmsr_logs/{args.experiment}_*.log')
    print(f'  2. å¯åŠ¨TensorBoard: tensorboard --logdir=logs/{args.experiment} --port=6006')
    print(f'  3. æŸ¥çœ‹æ ·æœ¬JSON: ls logs/{args.experiment}/samples/')
    print('='*60)
