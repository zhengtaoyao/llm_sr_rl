#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬çš„LLM-SR GRPOè®­ç»ƒè„šæœ¬

ä½¿ç”¨ç¬¦åˆVERLè¦æ±‚çš„æ•°æ®é›†ï¼Œé¿å…æ‰€æœ‰é…ç½®å­—æ®µç¼ºå¤±é—®é¢˜ã€‚
"""

import os
import sys
import tempfile
from pathlib import Path
from typing import Dict, Any
from omegaconf import DictConfig, OmegaConf

# ä¿®å¤ vLLM å†…å­˜æ± å…¼å®¹æ€§é—®é¢˜
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,garbage_collection_threshold:0.6"

# æ·»åŠ VERLåˆ°è·¯å¾„
verl_path = str(Path(__file__).parent.parent / "verl")
if verl_path not in sys.path:
    sys.path.append(verl_path)

from verl.trainer.main_ppo import run_ppo


def create_llmsr_reward_file(output_dir: str) -> str:
    """åˆ›å»ºLLM-SRè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°æ–‡ä»¶"""
    
    reward_function_code = '''
"""
LLM-SRç¬¦å·å›å½’å¥–åŠ±å‡½æ•°

å¯¹ç”Ÿæˆçš„å‡½æ•°è¿›è¡ŒBFGSä¼˜åŒ–å¹¶è¿”å›è´ŸMSEä½œä¸ºå¥–åŠ±ã€‚
"""

import numpy as np
import pandas as pd
from scipy.optimize import minimize
import re

def compute_score(data_sources, solution_strs, ground_truths, extra_infos, **kwargs):
    """
    è®¡ç®—LLM-SRç¬¦å·å›å½’ä»»åŠ¡çš„å¥–åŠ±
    
    Args:
        data_sources: æ•°æ®æºåˆ—è¡¨
        solution_strs: æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆå­—ç¬¦ä¸²åˆ—è¡¨
        ground_truths: å‚è€ƒç­”æ¡ˆåˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        extra_infos: é¢å¤–ä¿¡æ¯åˆ—è¡¨
        
    Returns:
        rewards: å¥–åŠ±åˆ†æ•°åˆ—è¡¨
    """
    rewards = []
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    data_path = "data/oscillator1/train.csv"
    df = pd.read_csv(data_path)
    
    # æå–è¾“å…¥è¾“å‡º
    x_data = df['x'].values
    v_data = df['v'].values  
    a_data = df['a'].values
    
    for i, solution_str in enumerate(solution_strs):
        try:
            # æå–å‡½æ•°ä½“
            reward = evaluate_solution(solution_str, x_data, v_data, a_data)
            rewards.append(reward)
        except Exception as e:
            print(f"è¯„ä¼°è§£å†³æ–¹æ¡ˆæ—¶å‡ºé”™: {e}")
            rewards.append(0.0)  # é”™è¯¯æƒ…å†µç»™0åˆ†
    
    return rewards


def evaluate_solution(solution_str: str, x_data: np.ndarray, v_data: np.ndarray, a_data: np.ndarray) -> float:
    """è¯„ä¼°å•ä¸ªè§£å†³æ–¹æ¡ˆçš„è´¨é‡"""
    
    try:
        # æå–å‡½æ•°ä½“ï¼ˆç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼æå–ï¼‰
        # æŸ¥æ‰¾ return è¯­å¥ä¹‹å‰çš„å†…å®¹
        lines = solution_str.strip().split('\\n')
        function_body = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('def ') or line.startswith('"""') or line == '"""':
                continue
            if line and not line.startswith('#'):
                function_body.append(line)
        
        if not function_body:
            return 0.0
            
        # æ„å»ºå®Œæ•´çš„å‡½æ•°ä»£ç 
        full_function = f"""
import numpy as np

def equation(x, v, params):
    {chr(10).join(['    ' + line for line in function_body])}

def evaluate_equation(params, x_data, v_data, a_data):
    try:
        pred = equation(x_data, v_data, params)
        mse = np.mean((pred - a_data) ** 2)
        return mse
    except:
        return 1e6  # å¤§çš„æƒ©ç½šå€¼
"""
        
        # æ‰§è¡Œä»£ç å¹¶ä¼˜åŒ–å‚æ•°
        namespace = {}
        exec(full_function, namespace)
        
        # BFGSä¼˜åŒ–
        initial_params = np.ones(10)  # 10ä¸ªå‚æ•°
        result = minimize(
            lambda p: namespace['evaluate_equation'](p, x_data, v_data, a_data),
            initial_params,
            method='BFGS'
        )
        
        final_mse = result.fun
        
        # è¿”å›è´ŸMSEä½œä¸ºå¥–åŠ±ï¼ˆMSEè¶Šå°ï¼Œå¥–åŠ±è¶Šå¤§ï¼‰
        if np.isnan(final_mse) or np.isinf(final_mse) or final_mse > 1e3:
            return 0.0
        else:
            return -final_mse  # è´Ÿæ•°ï¼Œå› ä¸ºMSEè¶Šå°è¶Šå¥½
            
    except Exception as e:
        return 0.0  # å‡ºé”™æ—¶è¿”å›0
'''
    
    output_path = os.path.join(output_dir, "llmsr_reward.py")
    with open(output_path, 'w') as f:
        f.write(reward_function_code)
    
    print(f"âœ… åˆ›å»ºå¥–åŠ±å‡½æ•°: {output_path}")
    return output_path


def create_grpo_config_simple(
    model_path: str,
    dataset_path: str,
    reward_file_path: str,
    output_dir: str,
    **kwargs
) -> DictConfig:
    """
    åˆ›å»ºç®€åŒ–çš„GRPOé…ç½®ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
    """
    
    # ç®€åŒ–çš„æ‰¹é‡å¤§å°é…ç½®
    gpus = kwargs.get('gpus', 6)
    micro_batch_size_per_gpu = 1  # æœ€å°å€¼ä»¥èŠ‚çœå†…å­˜
    rollout_n = kwargs.get('rollout_n', 4)
    
    # è®¡ç®—æ‰¹é‡å¤§å°
    train_batch_size = 48  # å›ºå®šå°æ‰¹é‡
    ppo_mini_batch_size = 24  # å›ºå®šå°æ‰¹é‡
    
    print(f"ğŸ”§ ç®€åŒ–æ‰¹é‡é…ç½®:")
    print(f"  GPUæ•°é‡: {gpus}")
    print(f"  å¾®æ‰¹é‡/GPU: {micro_batch_size_per_gpu}")
    print(f"  è®­ç»ƒæ‰¹é‡: {train_batch_size}")
    print(f"  PPOå°æ‰¹é‡: {ppo_mini_batch_size}")
    
    config = {
        # ç®—æ³•é…ç½®
        "algorithm": {
            "adv_estimator": "grpo",
            "gamma": 1.0,
            "lam": 1.0,
            "norm_adv_by_std_in_grpo": True,
            "use_kl_in_reward": False,
            "kl_ctrl": {
                "type": "fixed",
                "kl_coef": 0.001,
                "horizon": 10000,
                "target_kl": 0.1
            }
        },
        
        # æ•°æ®é…ç½®
        "data": {
            "train_files": [dataset_path],
            "val_files": [dataset_path],
            "train_batch_size": train_batch_size,
            "val_batch_size": ppo_mini_batch_size,
            "max_prompt_length": 1024,
            "max_response_length": 512,
            "filter_overlong_prompts": True,
            "truncation": "error",
            "reward_fn_key": "data_source",
            "shuffle": True
        },
        
        # Actor-Rollout-Refé…ç½®
        "actor_rollout_ref": {
            "model": {
                "path": model_path,
                "tokenizer_only": False,
                "use_remove_padding": True,
                "enable_gradient_checkpointing": True,
                "model_dtype": "bf16"
            },
            "actor": {
                "strategy": "fsdp",
                "optim": {
                    "lr": kwargs.get('learning_rate', 1e-6),
                    "eps": 1e-8,
                    "weight_decay": 0.01
                },
                "ppo_mini_batch_size": ppo_mini_batch_size,
                "ppo_micro_batch_size": None,
                "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,
                "use_kl_loss": True,
                "kl_loss_coef": 0.001,
                "kl_loss_type": "low_var_kl",
                "entropy_coeff": 0.0,
                "entropy_from_logits_with_chunking": False,
                "clip_ratio": 0.2,
                "ppo_epochs": 1,
                "loss_agg_mode": "token-mean",
                "use_dynamic_bsz": True,
                "ulysses_sequence_parallel_size": 1,
                "use_remove_padding": True,
                "use_fused_kernels": False,
                "use_torch_compile": True,
                "entropy_checkpointing": False,
                "grad_clip": 1.0,
                "shuffle": False,
                "data_loader_seed": None,
                "policy_loss": {"loss_mode": "vanilla"},
                "fsdp_config": {
                    "fsdp_size": gpus,
                    "param_offload": True,
                    "optimizer_offload": True,
                    "forward_prefetch": False,
                    "wrap_policy": {"min_num_params": 0}
                },
                "checkpoint": {
                    "save_contents": ["model", "optimizer", "extra"],
                    "load_contents": ["model", "optimizer", "extra"]
                }
            },
            "ref": {
                "log_prob_micro_batch_size": None,
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,
                "ulysses_sequence_parallel_size": 1,
                "log_prob_use_dynamic_bsz": True,
                "log_prob_max_token_len_per_gpu": 4096,
                "use_remove_padding": True,
                "use_fused_kernels": False,
                "entropy_from_logits_with_chunking": False,
                "use_torch_compile": True,
                "entropy_checkpointing": False,
                "grad_clip": 1.0,
                "fsdp_config": {
                    "forward_prefetch": False,
                    "param_offload": True,
                    "wrap_policy": {"min_num_params": 0}
                }
            },
            "rollout": {
                "name": "vllm",
                "mode": "sync",
                "n": rollout_n,
                "max_new_tokens": 512,
                "load_format": "auto",
                "dtype": "bfloat16",
                "prompt_length": 1024,
                "response_length": 512,
                "max_model_len": 4096,
                "enforce_eager": True,
                "enable_prefix_caching": False,
                "disable_log_stats": False,
                "enable_chunked_prefill": False,
                "disable_custom_all_reduce": True,
                "gpu_memory_utilization": 0.4,
                "max_num_batched_tokens": 4096,
                "seed": 0,
                "tensor_model_parallel_size": 1,
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 30,
                "log_prob_micro_batch_size": None,
                "log_prob_micro_batch_size_per_gpu": micro_batch_size_per_gpu,
                "val_kwargs": {
                    "do_sample": True,
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "top_k": 30,
                    "max_new_tokens": 512,
                    "n": 1
                },
                # ğŸ”¥ æ·»åŠ æ‰€æœ‰ç¼ºå¤±çš„å¿…éœ€å­—æ®µ
                "calculate_log_probs": False,
                "free_cache_engine": True
            }
        },
        
        # Criticé…ç½®
        "critic": {
            "strategy": "fsdp",
            "ppo_mini_batch_size": ppo_mini_batch_size,
            "ppo_micro_batch_size": None,
            "ppo_micro_batch_size_per_gpu": micro_batch_size_per_gpu,
            "use_dynamic_bsz": True
        },
        
        # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        "custom_reward_function": {
            "path": reward_file_path,
            "name": "compute_score"
        },
        
        # å¥–åŠ±æ¨¡å‹é…ç½®
        "reward_model": {
            "enable": False,
            "launch_reward_fn_async": False
        },
        
        # è®­ç»ƒå™¨é…ç½®
        "trainer": {
            "total_epochs": kwargs.get('epochs', 3),
            "total_training_steps": None,
            "project_name": "llm_sr_grpo_fixed",
            "experiment_name": kwargs.get('experiment_name', 'oscillator1_fixed'),
            "logger": ["console"],
            "n_gpus_per_node": gpus,
            "nnodes": 1,
            "save_freq": 2,
            "test_freq": 1,
            "val_before_train": True,
            "default_local_dir": output_dir,
            "device": "cuda",
            "resume_mode": "disable"
        },
        
        # Rayé…ç½®
        "ray_init": {
            "num_cpus": None
        }
    }
    
    return OmegaConf.create(config)


def train_llmsr_grpo_fixed(
    model_path: str = "/storage/home/westlakeLab/zhangjunlei/Qwen/Qwen2.5-Coder-7B-Instruct",
    output_dir: str = "./llmsr_grpo_outputs/oscillator1_fixed",
    **kwargs
):
    """
    ä¿®å¤ç‰ˆæœ¬çš„LLM-SR GRPOè®­ç»ƒ
    
    ä½¿ç”¨é¢„ç”Ÿæˆçš„ç¬¦åˆVERLè¦æ±‚çš„æ•°æ®é›†ï¼Œé¿å…æ‰€æœ‰é…ç½®é—®é¢˜ã€‚
    """
    
    print("ğŸ”¥ å¯åŠ¨ä¿®å¤ç‰ˆæœ¬çš„LLM-SR GRPOè®­ç»ƒ")
    print(f"ğŸ¤– æ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡º: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨é¢„ç”Ÿæˆçš„æ•°æ®é›†
    dataset_path = "llmsr_grpo_outputs/oscillator1_train.parquet"
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}. è¯·å…ˆè¿è¡Œ create_grpo_dataset.py")
    
    print(f"ğŸ“Š ä½¿ç”¨æ•°æ®é›†: {dataset_path}")
    
    # åˆ›å»ºå¥–åŠ±å‡½æ•°
    print("\nğŸ¯ åˆ›å»ºå¥–åŠ±å‡½æ•°...")
    reward_file_path = create_llmsr_reward_file(output_dir)
    
    # åˆ›å»ºGRPOé…ç½®
    print("\nâš™ï¸ åˆ›å»ºGRPOé…ç½®...")
    config = create_grpo_config_simple(
        model_path=model_path,
        dataset_path=dataset_path,
        reward_file_path=reward_file_path,
        output_dir=output_dir,
        **kwargs
    )
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(output_dir, "grpo_config_fixed.yaml")
    OmegaConf.save(config, config_path)
    print(f"ğŸ’¾ ä¿å­˜é…ç½®: {config_path}")
    
    # å¯åŠ¨è®­ç»ƒ
    print("\nğŸš€ å¯åŠ¨GRPOè®­ç»ƒ...")
    try:
        run_ppo(config)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    train_llmsr_grpo_fixed(
        epochs=2,
        learning_rate=1e-6,
        rollout_n=2,
        gpus=6,
        experiment_name="oscillator1_fixed_test"
    ) 