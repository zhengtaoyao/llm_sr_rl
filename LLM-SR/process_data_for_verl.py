#!/usr/bin/env python3
"""
ğŸ”¥ LLM-SRæ•°æ®é›†å¤„ç†å™¨ - ä¸ºVERL GRPOè®­ç»ƒå‡†å¤‡æ•°æ®

è¿™ä¸ªè„šæœ¬å¤„ç†@/dataç›®å½•ä¸­çš„æ‰€æœ‰æ•°æ®é›†ï¼Œå°†å…¶è½¬æ¢ä¸ºç¬¦åˆVERLæ ¼å¼è¦æ±‚çš„æ•°æ®ï¼Œ
ç¡®ä¿åŒ…å«å¿…éœ€çš„`reward_model.ground_truth`å­—æ®µä»¥é¿å…KeyErrorã€‚

ç”¨æ³•:
    python process_data_for_verl.py
    
è¾“å‡º:
    åœ¨verl_datasets/ç›®å½•ä¸­ç”Ÿæˆæ‰€æœ‰é—®é¢˜çš„parquetæ ¼å¼æ•°æ®é›†
"""

import os
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
import re


def extract_function_names(specification: str) -> Tuple[str, str]:
    """ä»specificationæ¨¡æ¿ä¸­æå–å‡½æ•°å"""
    
    # æŸ¥æ‰¾ @evaluate.run è£…é¥°çš„å‡½æ•°
    run_pattern = r'@evaluate\.run\s+def\s+(\w+)'
    run_match = re.search(run_pattern, specification)
    run_function = run_match.group(1) if run_match else "evaluate_function"
    
    # æŸ¥æ‰¾ @equation.evolve è£…é¥°çš„å‡½æ•°  
    evolve_pattern = r'@equation\.evolve\s+def\s+(\w+)'
    evolve_match = re.search(evolve_pattern, specification)
    evolve_function = evolve_match.group(1) if evolve_match else "symbolic_regression"
    
    return evolve_function, run_function


def build_problem_prompt(template_content: str, problem_name: str) -> str:
    """æ ¹æ®æ¨¡æ¿æ„å»ºé—®é¢˜çš„æç¤ºè¯"""
    
    # æå–æ¨¡æ¿ä¸­@equation.evolveå‡½æ•°ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºåŸºç¡€æç¤º
    lines = template_content.split('\n')
    prompt_lines = []
    in_evolve_function = False
    
    for line in lines:
        if '@equation.evolve' in line:
            in_evolve_function = True
            continue
        if in_evolve_function and line.strip().startswith('def '):
            # æ‰¾åˆ°å‡½æ•°å®šä¹‰ï¼ŒåŒ…å«å®ƒå¹¶åœæ­¢
            prompt_lines.append(line.rstrip())
            break
        if not in_evolve_function:
            prompt_lines.append(line.rstrip())
    
    base_prompt = '\n'.join(prompt_lines).strip()
    
    # æ ¹æ®é—®é¢˜æ·»åŠ ç‰¹å®šçš„æŒ‡å¯¼
    if 'oscillator' in problem_name:
        task_description = """
ä½ çš„ä»»åŠ¡æ˜¯å‘ç°æè¿°ç®€è°æŒ¯è¡å™¨è¿åŠ¨çš„ç‰©ç†æ–¹ç¨‹ã€‚
ç»™å®šä½ç½®xã€é€Ÿåº¦vï¼Œé¢„æµ‹åŠ é€Ÿåº¦aã€‚
è¯·ä½¿ç”¨ç‰©ç†å­¦çŸ¥è¯†ï¼Œè€ƒè™‘èƒ¡å…‹å®šå¾‹å’Œç®€è°è¿åŠ¨çš„ç‰¹æ€§ã€‚
"""
    elif 'bactgrow' in problem_name:
        task_description = """
ä½ çš„ä»»åŠ¡æ˜¯å‘ç°æè¿°ç»†èŒç”Ÿé•¿é€Ÿç‡çš„ç”Ÿç‰©å­¦æ–¹ç¨‹ã€‚
ç»™å®šç»†èŒæµ“åº¦bã€åº•ç‰©æµ“åº¦sã€æ¸©åº¦tempã€pHå€¼ï¼Œé¢„æµ‹ç»†èŒç”Ÿé•¿é€Ÿç‡dbã€‚
è¯·è€ƒè™‘å¾®ç”Ÿç‰©å­¦ä¸­çš„Monodæ–¹ç¨‹ã€æ¸©åº¦å’ŒpHå¯¹ç”Ÿé•¿çš„å½±å“ã€‚
"""
    elif 'stressstrain' in problem_name:
        task_description = """
ä½ çš„ä»»åŠ¡æ˜¯å‘ç°æè¿°ææ–™åº”åŠ›-åº”å˜å…³ç³»çš„å·¥ç¨‹æ–¹ç¨‹ã€‚
è¯·è€ƒè™‘ææ–™åŠ›å­¦ä¸­çš„èƒ¡å…‹å®šå¾‹ã€å¡‘æ€§å˜å½¢ç­‰æ¦‚å¿µã€‚
"""
    else:
        task_description = """
ä½ çš„ä»»åŠ¡æ˜¯é€šè¿‡ç¬¦å·å›å½’å‘ç°æ•°æ®ä¸­éšè—çš„æ•°å­¦æ–¹ç¨‹ã€‚
è¯·åˆ†æè¾“å…¥å˜é‡å’Œè¾“å‡ºå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºåˆç†çš„æ•°å­¦è¡¨è¾¾å¼ã€‚
"""
    
    return base_prompt + task_description


def get_ground_truth_equations() -> Dict[str, str]:
    """è·å–å„ä¸ªé—®é¢˜çš„ç†è®ºæ–¹ç¨‹ï¼ˆä½œä¸ºground truthï¼‰"""
    
    return {
        'oscillator1': 'a = -x',  # ç®€è°æŒ¯è¡å™¨: F = -kx, a = F/m = -kx (å‡è®¾k/m=1)
        'oscillator2': 'a = -x - 0.1*v',  # å¸¦é˜»å°¼çš„æŒ¯è¡å™¨
        'bactgrow': 'db = r * b * s / (K + s) * f(temp) * g(pH)',  # Monodæ–¹ç¨‹
        'stressstrain': 'stress = E * strain'  # èƒ¡å…‹å®šå¾‹
    }


def create_verl_dataset_entry(
    prompt: str, 
    problem_name: str, 
    data_sample: Dict[str, float], 
    ground_truth: str,
    data_source: str = "llm_sr_train"
) -> Dict[str, Any]:
    """åˆ›å»ºå•ä¸ªVERLæ•°æ®é›†æ¡ç›®"""
    
    # æ„å»ºå¯¹è¯æ ¼å¼çš„æç¤ºï¼ˆLLMéœ€è¦chatæ ¼å¼ï¼‰
    chat_messages = [
        {
            "role": "user", 
            "content": prompt
        }
    ]
    
    # åˆ›å»ºVERLæ ¼å¼çš„æ•°æ®æ¡ç›®ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
    entry = {
        "prompt": chat_messages,
        "data_source": data_source,
        "ability": f"symbolic_regression_{problem_name}",  # æ·»åŠ èƒ½åŠ›æ ‡è¯†
        "extra_info": {
            "problem_type": problem_name,
            "task": "symbolic_regression",
            "domain": "scientific_equation_discovery",
            "data_sample": data_sample  # åŒ…å«å…·ä½“çš„æ•°æ®æ ·æœ¬
        },
        "reward_model": {
            "style": "rule",  # ä½¿ç”¨åŸºäºè§„åˆ™çš„è¯„ä¼°
            "ground_truth": ground_truth,  # ğŸ”¥ å…³é”®ï¼šæä¾›ground truthé¿å…KeyError
            "evaluation_type": "mse_based",  # åŸºäºMSEçš„è¯„ä¼°
            "target_variables": list(data_sample.keys())  # ç›®æ ‡å˜é‡
        }
    }
    
    return entry


def process_single_dataset(
    problem_name: str, 
    data_dir: Path, 
    spec_dir: Path,
    output_dir: Path,
    max_samples: int = 1000
) -> bool:
    """å¤„ç†å•ä¸ªæ•°æ®é›†"""
    
    print(f"\nğŸ”„ å¤„ç†æ•°æ®é›†: {problem_name}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    train_file = data_dir / problem_name / "train.csv"
    spec_file = spec_dir / f"specification_{problem_name}_numpy.txt"
    
    if not train_file.exists():
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_file}")
        return False
        
    if not spec_file.exists():
        print(f"âŒ è§„èŒƒæ–‡ä»¶ä¸å­˜åœ¨: {spec_file}")
        return False
    
    # è¯»å–æ•°æ®å’Œæ¨¡æ¿
    try:
        df = pd.read_csv(train_file)
        with open(spec_file, 'r', encoding='utf-8') as f:
            template_content = f.read()
            
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False
    
    # æå–å‡½æ•°åå’Œæ„å»ºæç¤º
    evolve_function, run_function = extract_function_names(template_content)
    prompt = build_problem_prompt(template_content, problem_name)
    
    # è·å–ground truth
    ground_truths = get_ground_truth_equations()
    ground_truth = ground_truths.get(problem_name, f"æœªçŸ¥æ–¹ç¨‹ï¼Œéœ€è¦é€šè¿‡{evolve_function}å‡½æ•°å‘ç°")
    
    print(f"ğŸ¯ ç›®æ ‡å‡½æ•°: {evolve_function}")
    print(f"ğŸ” è¯„ä¼°å‡½æ•°: {run_function}")
    print(f"ğŸ“œ ç†è®ºæ–¹ç¨‹: {ground_truth}")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜è®­ç»ƒæ•ˆç‡
    num_samples = min(max_samples, len(df))
    df_sampled = df.sample(n=num_samples, random_state=42).reset_index(drop=True)
    
    print(f"ğŸ“Š é€‰æ‹©æ ·æœ¬æ•°: {num_samples}")
    
    # åˆ›å»ºVERLæ ¼å¼çš„æ•°æ®æ¡ç›®
    dataset_entries = []
    
    for idx, row in df_sampled.iterrows():
        # å°†æ•°æ®è¡Œè½¬æ¢ä¸ºå­—å…¸
        data_sample = row.to_dict()
        
        # åˆ›å»ºæ•°æ®æ¡ç›®
        entry = create_verl_dataset_entry(
            prompt=prompt,
            problem_name=problem_name,
            data_sample=data_sample,
            ground_truth=ground_truth,
            data_source=f"llm_sr_{problem_name}_train"
        )
        
        dataset_entries.append(entry)
    
    # ä¿å­˜ä¸ºparquetæ ¼å¼
    output_file = output_dir / f"{problem_name}_train_verl.parquet"
    
    try:
        # è½¬æ¢ä¸ºPyArrowè¡¨æ ¼å¹¶ä¿å­˜
        table = pa.Table.from_pylist(dataset_entries)
        pq.write_table(table, output_file)
        
        print(f"âœ… æˆåŠŸä¿å­˜: {output_file}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.1f} KB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†æ‰€æœ‰æ•°æ®é›†"""
    
    print("ğŸ”¥ LLM-SR æ•°æ®é›†å¤„ç†å™¨ - ä¸ºVERL GRPOè®­ç»ƒå‡†å¤‡æ•°æ®")
    print("=" * 70)
    
    # è®¾ç½®è·¯å¾„
    base_dir = Path(".")
    data_dir = base_dir / "data"
    spec_dir = base_dir / "specs"
    output_dir = base_dir / "verl_datasets"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
        
    if not spec_dir.exists():
        print(f"âŒ è§„èŒƒç›®å½•ä¸å­˜åœ¨: {spec_dir}")
        return
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„é—®é¢˜
    available_problems = []
    for item in data_dir.iterdir():
        if item.is_dir() and (item / "train.csv").exists():
            available_problems.append(item.name)
    
    print(f"ğŸ” å‘ç°çš„æ•°æ®é›†: {available_problems}")
    
    if not available_problems:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ•°æ®é›†")
        return
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    success_count = 0
    total_count = len(available_problems)
    
    for problem_name in available_problems:
        try:
            success = process_single_dataset(
                problem_name=problem_name,
                data_dir=data_dir,
                spec_dir=spec_dir,
                output_dir=output_dir,
                max_samples=1000  # é™åˆ¶æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°
            )
            
            if success:
                success_count += 1
                
        except Exception as e:
            print(f"âŒ å¤„ç†{problem_name}æ—¶å‡ºé”™: {e}")
            continue
    
    # æ€»ç»“
    print(f"\n{'='*70}")
    print(f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count}/{total_count} ä¸ªæ•°æ®é›†")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    if success_count > 0:
        print(f"\nğŸ“‹ å¯ç”¨çš„VERLæ•°æ®é›†:")
        for file in output_dir.glob("*_train_verl.parquet"):
            file_size = file.stat().st_size / 1024
            print(f"  - {file.name} ({file_size:.1f} KB)")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥: ä¿®æ”¹run_llmsr_grpo_direct.shä½¿ç”¨æ–°çš„æ•°æ®é›†è·¯å¾„")
        print(f"   ä¾‹å¦‚: --dataset_path verl_datasets/oscillator1_train_verl.parquet")
    else:
        print(f"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®é›†")


if __name__ == "__main__":
    main() 